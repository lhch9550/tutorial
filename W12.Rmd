---
---
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(width=700)
```

# 13, Part 1. Cross-Sectional Network Models: ERGM {#ch13-Cross-Sectional-Network-Models-ERGM-R .unnumbered}

This tutorial offers an extended example in R demonstrating how to analyze networks using statistical models. We will focus on exponential-family random graph models (ERGMs). ERGMs are useful as they make it possible to uncover what micro tendencies are important in network formation, comparing rates of reciprocity, homophily (for example) net of other network processes. The tutorial will draw on many of the previous tutorials, most directly the tutorials on dyads/triads ([Chapter 7](#ch7-Dyads-Triads-R)) and centrality/hierarchy ([Chapter 9](#ch9-Network-Centrality-Hierarchy-R)). The goals of the tutorial are to get acquainted with: model fitting, interpretation, diagnostics, model comparison, and simulating from a known model. We will offer two main examples, one based on a small school-based network, and one based on a much larger coauthorship network. Both examples are based on cross-sectional network data (so one time point).

## Setting up the Session

We will utilize a small school-based network for the first part of the ERGM tutorial. Our main question is what micro-level processes are important for the formation of the network. In particular, we want to explore some of the processes around gender and grade. For example, do boys tend to have more friends than girls? Is gender homophily a strong driver of tie formation? Is gender homophily stronger than grade homophily? How does controlling for other micro tendencies, like reciprocity and transitive closure, change our conclusions?

First, let's load the necessary packages.

```{r message=F, warning=F}
#install.packages("ergm")
#install.packages("ergm.count")
```

```{r message=F, warning=F}
library(ergm)
library(sna)
library(car)
```

Note that the **ergm** package [@ergm] uses network objects consistent with the **network** package. It is also useful to have the **latticeExtra** and **Rglpk** packages installed.

Now, let's read in our example network data representing a high school friendship network. The network data are saved as an adjacency matrix:

```{r}
url1 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/example_schoolmat.csv"

school_mat <- read.csv(file = url1, row.names = 1) 
```

We have added row.names = 1 to tell R that the first column should be used to set the row names. Let’s turn that data frame into a matrix.

```{r}
school_mat <- as.matrix(school_mat)
```

And now let’s take look at the matrix (just the first five rows and columns). A 1 indicates that person i named person j as a friend.

```{r}
# Updated the slicing range from school_mat[1:5, 1:5] to school_mat[1:10, 1:10]
school_mat[1:10, 1:10] 
```

Now, we will read in the attribute file, containing nodal characteristics, including gender, grade and ses.

```{r}
url2 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/example_schooldata.csv"

school_attributes <- read.csv(file = url2) 
```

```{r}
# Display all metadata attributes of the school_attributes data frame
attributes(school_attributes)
head(school_attributes)
```

Let's make it easier to interpret the gender variable. Let's recode the 0s and 1s into male and female using a `recode()` function.

```{r}
school_attributes$gender_recode <- recode(school_attributes$gender, 
                                          as.factor = F, 
                                          "0 = 'male'; 1 = 'female'")
```

Let's construct a network object and put the attributes onto the network. First, we need to create a list of attributes based on our attribute data frame.

```{r}
attribute_list <- do.call(list, school_attributes) 
```

Now we create the network based on the matrix and attribute list. We set the network as directed.

```{r}
school_net <- network(x = school_mat, directed = T, 
                      vertex.attr = attribute_list) 
```

```{r}
# Visualize the directed network with default layout
plot(school_net,
     displaylabels = TRUE,   # show vertex labels
     label.cex = 0.7,        # label size
     main = "School Network")  # plot title
```

## Descriptive Statistics

Before we go ahead and try to fit an exponential random graph model, it will be useful to know a bit about the network in question. Here we will plot the network and calculate basic network statistics.

```{r fig.height=7.5, fig.width=7.5}
plot(school_net)
```

One main question is how gender and grade shape the formation of ties in the network. So, let's look at the network but color the nodes by gender and then by grade. Let's color the nodes by gender, blue for boys and pink for girls:

```{r}
cols <- recode(school_attributes$gender, as.factor = F,
               "0 = 'blue'; 1 = 'pink'")
```

We will now use the cols vector in our plot statement (using the default `plot()` function in the **network** package).

```{r fig.height=7.5, fig.width=7.5}
plot(school_net, vertex.col = cols) 
```

It looks like there is moderate homophily along gender lines. What about grade?

```{r fig.height=7.5, fig.width=7.5}
plot(school_net, vertex.col = school_attributes$grade) 
```

We see pretty clear social divisions by grade. It is also useful to know a bit about indegree and outdegree, seeing which groups (if any) are particularly likely to have high degree. Let's calculate indegree and outdegree for the network:

```{r}
outdegree <- degree(school_net, cmode = "outdegree")
indegree <- degree(school_net, cmode = "indegree")
```

Let's see if boys or girls have higher degree. We will check this by calculating mean indegree by gender. We can use `tapply()`, with indegree as the first input, gender as the second, and the function (here mean) as the third.

```{r}
tapply(indegree, school_attributes$gender_recode, mean) 

# Boxplot showing the distribution of indegree by gender
boxplot(indegree ~ school_attributes$gender_recode,
        main = "Distribution of Indegree by Gender",
        xlab = "Gender",
        ylab = "Indegree",
        col = c("lightblue", "pink"))
```

Here we calculate the same thing for outdegree.

```{r}
tapply(outdegree, school_attributes$gender_recode, mean) 

# Boxplot showing the distribution of outdegree by gender
boxplot(outdegree ~ school_attributes$gender_recode,
        main = "Distribution of Outdegree by Gender",
        xlab = "Gender",
        ylab = "Outdegree",
        col = c("lightblue", "lightpink"))
```

It looks like boys receive and send out more ties than girls.

## CUG Tests

We now turn to some simple statistical tests about the features of the network. This will give us useful information about the network, setting our base expectations before running the full model below. Here we will focus on Conditional Uniform Graph Tests (CUG test). See also [Chapter 7](#ch7-Dyads-Triads-R).

We begin by calculating the features of interest, like reciprocity or transitivity. We will then compare the observed statistic with the values that one would see in a random network of the same size (and possibly other constraints). As a researcher, you specify the baseline model of interest, specifying how to construct the random network. This sets the comparison of interest, effectively holding certain desired features constant and allowing others to vary randomly. The question is whether the observed statistic is above/below (or similar to) what one would expect by chance. Let's walk through a few examples to get a sense of the features of our network and how this compares to a random network. First, let's calculate density on the network.

```{r}
gden(school_net) 
```

We can see that .125 of all possible ties exist. Now, let's compare the observed density to what we are likely to see in a random network of the same size (i.e., if we took a network of the same size and just started randomly forming ties with probability .5, what would the density look like?). Here, we will run the CUG test with the function of interest set to "gden", calculating density. The conditioning is set to "size", holding size constant at the true network size. Everything else is allowed to vary randomly. We set dat to our network of interest.

```{r}
cug.test(dat = school_net, FUN = "gden", cmode = "size") 
```

The results clearly show that the density of .125 is well below what we would expect in a random network of size 32 (as the probability of finding simulated values above the observed value is effectively 1). This shouldn't be surprising however, as people have a limited capacity to form ties and the number of possible ties increases non-linearly as size goes up. Density thus tends to decrease non-linearly as system size increases.

Now, let's do something a little more interesting and look at the rate of reciprocity. Reciprocity captures how often i is tied to j when j is tied to i. Here, we will condition the randomly generated networks on density. The question is thus: is the observed rate of reciprocity higher/lower than what we would expect in a network of the same size and number of edges, but where the edges are formed randomly? First, let's calculate reciprocity on the observed network, not including the null dyads in the calculation.

```{r}
grecip(school_net, measure = "dyadic.nonnull")
```

This says that about .409 of the non-null dyads are reciprocated (where i-\>j and j-\>i). And now we will run a CUG test. Here the function of interest is grecip, calculating reciprocity; the argument to the function is measure, set to "dyadic.nonnull". This is passed to grecip via FUN.args. cmode is set to "edges" to condition the random graphs on density.

```{r}
cug.test(dat = school_net, FUN = "grecip", 
         FUN.args = list(measure = "dyadic.nonnull"), cmode = "edges") 
```

The results suggest that the reciprocity rate, .409, is above what we would expect in a network of the same size and number of edges but otherwise random ties. We can see this as the probability of finding a randomly generated value higher than the observed value is 0. This would suggest that reciprocity is an important micro process shaping the formation of ties. We can do similar analyses with transitivity (or other statistics), constraining the randomly formed networks to make useful comparisons. For example, it may be useful to ask if the rate of transitivity is higher than what we would expect in a network with the same dyad census, showing if there are triadic effects (if i is friends with j and j is friends with k, is i friends with k?), above what can be induced by dyadic processes (i.e., reciprocity).

## Fitting an ERGM

We have so far learned a bit about our network. We know there is strong grade homophily and weaker gender homophily. We know that boys tend to give and receive more ties than girls. We also know that there are likely norms around reciprocating friendship. We now want to consider these micro-processes together, to see (in a systematic way) how important each factor is in predicting the existence of a tie, net of the other network processes. We saw a very simple version of this above with the CUG test (looking at the rate of reciprocity net of density) but we want to consider a number of factors together. We also want to see how these factors work together to generate the larger features of the network, like distance, thus linking the micro-processes to the emergent, global features of the network.

An exponential random graph model allows us to take the network as the item of interest. The model predicts the presence/absence of a tie between all i-j pairs as a function of different network features, like reciprocity and homophily (on gender for example). It is useful to think of the terms added to the model in terms of dependencies. By including a term for reciprocity (for example), one is claiming that the existence of the i-\>j tie is dependent on whether or not j-\>i exists. We can apply similar logic to other terms, where different terms correspond to different arguments about what the formation of the i-\>j tie is (or is not) dependent on.

Before we actually fit any ERGMs, it will be useful to look at some of the help files for the main functions: `?ergm`, `?'ergm-terms'`, `?mcmc.diagnostics`, `?gof`.

The function to actually run the model is `ergm()`. There are a large number of possible arguments to ergm. We focus on the main ones here:

-   formula = a formula object with network on the left followed by a \~ and then the set of terms we want to include in the model.
-   control = list of options used in the algorithm, like sample size and burnin. The inputs are set via a `control.ergm()` function.
-   constraints = a formula dictating if there should be any constraints on the networks generated in the MCMC sample (used to estimate the parameters). This can be useful when certain networks are impossible (i.e., because the survey constrained the number of friends the respondents could name). For example: `constraints = ~ bd(maxout = 10)` would only consider networks where the max outdegree is 10 or less.

### Edges Only

We will start simple and run an ERGM with only edges as a predictor. We thus predict the school network as a function of the number of edges in the network and nothing else.

```{r, message=F, results='hide'}
mod_rand <- ergm(formula = school_net ~ edges) 
```

Let’s take a look at the model results.

```{r}
summary(mod_rand) 
```

The interpretation of the model coefficients is straightforward in this case. The log-odds of any tie existing is -1.946. The probability of any tie existing is: `exp(-1.946) / (1 + exp(-1.946))` = .125, which is just the density. Note that if we do a summary on the ergm formula, we get the counts for each network statistic in the observed network.

```{r}
summary(school_net~edges) 
```

We see here that there are 124 edges in the observed network.

### Edges and Homophily

We know that homophily matters from the plots, so let's add nodematch terms for gender and grade. The nodematch terms count the number of times that an edge exists where i and j have the same attribute (here gender or grade). Note that the **ergm** package does not currently handle missing data on nodal attributes; the simplest solution to such issues is to impute missing attribute data prior to model estimation.

```{r message=F, warning=F}
mod_homoph1 <- ergm(school_net ~ edges + nodematch("gender_recode") + 
                      nodematch("grade")) 
```

```{r}
summary(mod_homoph1)
```

The results suggest that there is strong homophily on grade, well above that expected by chance (conditioned on density and homophily on gender). Grade thus appears to be an important factor in the formation of ties in the network. The nodematch term on gender is not significant, suggesting that the number of edges that match on gender is not clearly above what we would expect in a random network of the same size (conditioned on the other terms in the model, including density).

Let's interpret our grade nodematch coefficient. For a tie between two nodes that match on grade and mismatch on gender the conditional log-odds is: `-2.6140 + .1137 * 0 + 2.3088 * 1` = -.3052. The probability of a tie is: `exp(-.3052) / (1 + exp(-.3052))` = .424. For a tie between two nodes that mismatch on grade and mismatch on gender the conditional log-odds is: `-2.6140 + .1137 * 0 + 2.3088 * 0` = -2.6140. The probability of a tie is: `exp(-2.6140) / (1 + exp(-2.6140))` = .068. We can see that the probability of a tie goes down substantially when the two nodes are not in the same grade. In terms of odds ratios, we can say that the odds of a tie existing is `exp(2.3088)` = 10.062 times higher if the nodes match on grade than if they differ on grade, net of the other terms in the model.

Note that we would want to use an absdiff term if the attribute of interest is continuous and can take on many values, such as with gpa or household income. absdiff takes the absolute difference between the two actors in the dyad on the attribute of interest. The basic idea is that actors who are tied together may have similar values but not match exactly. absdiff is coded in the opposite manner as nodematch, so that a negative coefficient is evidence of homphily.

### Edges, Homophily, and Node Covariates

We can ask related questions about gender and grade by adding terms to the model that show if boys (compared to girls) send out and/or receive more ties. We will add nodeifactor terms and nodeofactor terms for gender. nodeifactor captures the ties coming in and nodeofactor captures the ties going out. The terms capture if the mean number of ties coming in or going out is higher for boys than girls. We can add analogous terms for grade using nodeicov and nodeocov. Nodecov terms are appropriate for quantitative variables, rather than factors.

```{r message=F, warning=F}
mod_homoph2 <- ergm(school_net ~ edges + nodematch("gender_recode") +
                      nodematch("grade") + 
                      nodeifactor("gender_recode") + 
                      nodeofactor("gender_recode") +
                      nodeicov("grade") + nodeocov("grade"))
```

```{r}
summary(mod_homoph2) 
```

It looks like individuals who are male are more likely to send and receive friendship nominations, net of the other terms in the model. The results also suggest that individuals in higher grades tend to receive and send more ties while forming strong boundaries around grade (looking at the nodematch term on grade).

### Edges, Homophily, Node Covariates, and Reciprocity

Now, let's go ahead and make a more complicated model, one that incorporates reciprocity. We will do this by adding a mutual term to the model. The mutual term counts the number of pairs where i-\>j and j-\>i exist. Note that by adding mutual to the model the model will now be fit using MCMC methods. With MCMC estimation, it is often useful to set the input parameters to the estimation routine. This can be done using `control.ergm()`. Let's see how to do this. Here we will set the sample size using a MCMC.samplesize argument and burnin using a MCMC.burnin argument.

```{r message=F, warning=F}

# Adjusted the followwing parameters
# MCMC.burnin (50,000 -> 10,000): Longer burn-in helps the chain reach the target distribution and stabilize convergence.
# MCMC.samplesize (6,000 -> 8,000): Larger sample size reduces Monte Carlo error and improves estimate precision.
set.seed(1012)

mod_homoph_mutual1 <- ergm(school_net ~ edges + 
                             nodematch("gender_recode") + 
                             nodematch("grade") + 
                             nodeifactor("gender_recode") + 
                             nodeofactor("gender_recode") + 
                             nodeicov("grade") + nodeocov("grade") +
                             mutual, 
                           control = control.ergm(MCMC.burnin = 100000,
                                                  MCMC.samplesize = 8000)) 
```

Note that in the R console there we will be added output telling you what the algorithm is doing, although we suppress this here.

## Checking Model Convergence

Our model is now being estimated via MCMC methods. It is important to make sure that the algorithm converged, so that the sampled networks (which the estimates are based on) offer reasonable, consistent statistics (or counts) sample to sample. Let's see how the model looks using a `mcmc.diagnostics()` function:

```{r results='hide'}
pdf("mcmc_diagnostics.pdf", width = 10, height = 8)
mcmc.diagnostics(mod_homoph_mutual1, vars.per.page = 4)
dev.off()

```

The function plots the MCMC sample of network statistics for each term included in the model. The plots show how far the statistics calculated on the sampled networks are from the true value, calculated on the observed network. The plots on the left hand show the statistics taken from each individual sample (plotted in order), while the right hand side shows the distribution of sample statistics. A 'nice' looking plot would have the sample statistics centered around 0, with some (but not extreme) variance sample to sample, suggesting that the specified model is well-behaved and can produce networks consistent with the observed data.

In this case everything looks okay and the model seems to have converged. The networks statistics haven't gone off dramatically in one direction and are not erratic, changing dramatically sample to sample (but there is still variation). We will consider examples later on where the models do not converge. Note that if we had found problems one possible option is to tweak the input parameters further, generally allowing for a longer burnin, more iterations between samples and so on. Let's go ahead and look at the results:

```{r message=F}
summary(mod_homoph_mutual1)
```

We can push this intuition a little further by allowing the effect of different relational terms, like mutual, to vary by nodal attributes. The idea is that certain interactional tendencies may be stronger/weaker for different sets of actors; for example, based on gender. This amounts to including an interaction in the model, between the term of interest and a nodal attribute. Here, we want to test if boys have more reciprocated ties than girls, and thus are involved in more mutual dyads. We will specify this by including a mutual term where we set by to the attribute of interest (`gender_recode`). See `?'ergm-terms'` for more options on the mutual term.

The model will include two mutual terms, one for boys and one for girls, counting the number of mutual dyads that include at least one boy (or girl). For this initial model, we will not include nodefactor terms for gender. We add those controls below as a means of comparison.

```{r message=F}
mod_homoph_mutual1b <- ergm(school_net ~ edges + 
                              nodematch("gender_recode") +
                              nodematch("grade") + 
                              nodeicov("grade") + nodeocov("grade") +
                              mutual(by = "gender_recode"),
                            control = control.ergm(MCMC.burnin = 50000, 
                                                   MCMC.samplesize = 6000))
```

```{r}
summary(mod_homoph_mutual1b)
```

We can see that boys are, in fact, more likely to be involved in mutual dyads than girls (comparing the coefficient on mutual for male to female). The results are thus consistent with our previous conclusions. Now, let's go ahead and include a control for degree differences (by gender) in the model. This will allow us to see if boys are more likely to be involved in mutual dyads because they send out more ties or because expectations of reciprocity are higher among boys. We will include the nodeofactor term for gender, controlling for the tendency to send out ties.

```{r message=F}
mod_homoph_mutual1c <- ergm(school_net ~ edges +
                              nodematch("gender_recode") + 
                              nodematch("grade") + 
                              nodeofactor("gender_recode") + 
                              nodeicov("grade") + nodeocov("grade") + 
                              mutual(by = "gender_recode"),
                            control = control.ergm(MCMC.burnin = 50000, 
                                                   MCMC.samplesize = 6000))
```

```{r}
summary(mod_homoph_mutual1c)
```

It looks like boys and girls have similar tendencies of forming mutual dyads, controlling for the rate of sending out ties. Thus, the differences between boys and girls are not in the expectations of reciprocity, but in the sheer volume of relationships that boys tend to be involved in.

Note that it is difficult to do formal statistical tests comparing one coefficient (mutual) across group (boys v. girls) in logistic regression. A more formal test would require using average marginal effects in the scale of the predicted probabilities. See the **ergMargins** package.

## Goodness of Fit

It is generally a good idea to see if the specified model is fitting the network well. The basic test is whether the micro-level tie formation processes assumed in the model are sufficient to reproduce the features of the network as a whole (e.g., if individuals form ties based on the specified micro-processes, does that explain why the network looks the way it does?). Note that this is a goodness of fit test and is different than the MCMC diagnostics viewed above. The MCMC diagnostics only show if the model estimates can be trusted and thus focus on the terms included in the model. Goodness of fit statistics, in contrast, test whether the model can reproduce features of the network not included in the model.

Here, we will ask if our model can reproduce distance and the shared partner distribution from the true network. Distance captures the distribution of shortest paths between all ij pairs. The shared partner distribution shows for each ij pair how many other nodes both i and j are friends with. This captures clustering in the network, showing if i and j tend to be friends with the same people. Let's take a quick look at the edgewise shared partner distribution in the observed network:

```{r}
summary(school_net ~ esp(0:10)) 
```

This says that 24 students who are friends have 0 friends in common, 37 have 1 friend in common, and so on. The function to run the goodness of fit test is gof. The inputs are the fitted model followed by the statistics of interest you want to test against, set using the GOF argument (as a formula). The `gof()` function will simulate a set of networks based on the estimated model; it will then take the generated networks, calculate the macro features of interest (here distance and the shared partner distribution) and compare that to the true value, based on the empirical network. We will also include a model term in the formula. This asks if the model is reproducing the terms included in the model itself (like mutual and homophily on grade). This is a useful check, although not a good test of model fit. We will use the model with the simple specification for mutual (i.e., not allowing the effect to vary by gender).

```{r}
gof_mod_homoph_mutual1 <- gof(mod_homoph_mutual1, 
                              GOF = ~ distance + espartners + model,
                              control = control.gof.ergm(seed = 110)) 
```

We first set up the plot to have three columns and then plot the goodness of fit statistics.

```{r}
par(mfrow = c(1, 3)) 
plot(gof_mod_homoph_mutual1)
```

The boxplots capture the values from the simulated network and the black line represent the values from the observed network. A good model will have the values from the simulated networks close to the true values. Our model is clearly missing something with the shared partners. We can see that the simulated networks greatly overestimate the number of (tied) pairs with 0 common partners. Or, more substantively, it is clear that the model underestimates local clustering (or the tendency for a friend of a friend to be a friend). We can also look at the goodness of fit statistics directly.

```{r}
gof_mod_homoph_mutual1
```

A significant p-value tells us that the observed value (from the true network) is significantly different from the values from the simulated networks, a clear sign that the model is not fitting well. For example, there is clear difference for the esp0 (0 shared partner) count. Note also that when doing the goodness of fit test we can include the same kinds of control and constraint inputs as with the actual model fitting.

Let's see if we can complicate our model a bit to account for local clustering. First, let's make a small tweak to the model, here changing the way we estimate homophily on grade. By adding diff = T to the grade nodematch term we add 6 terms to the model, one for each grade. The idea is that in-group bias may be stronger/weaker for certain grades.

```{r message=F}
mod_homoph_mutual2 <- ergm(school_net ~ edges + 
                             nodematch("gender_recode") + 
                             nodematch("grade", diff = T) + 
                             nodeifactor("gender_recode") + 
                             nodeofactor("gender_recode") + 
                             nodeicov("grade") + nodeocov("grade") + 
                             mutual)
```

```{r}
summary(mod_homoph_mutual2)
```

Here, we see that grade 12 has the highest in-group bias (where they disproportionately have friends within the same grade) while grade 8 has the lowest in-group bias. Let’s again look at goodness of fit.

```{r results='hide'}
gof_mod_homoph_mutual2 <- gof(mod_homoph_mutual2, 
                              GOF = ~ distance + espartners + model,
                              control = control.gof.ergm(seed = 113)) 
```

```{r results='hide'}
par(mfrow = c(1, 3)) 
plot(gof_mod_homoph_mutual2)
```

It doesn't look much better in terms of fit. Of course, it still may be of interest to know if there are different levels of in-group bias across grades.

## Adding GWESP to the Model

We have so far fit a number of models, learned about reciprocity and homophily but have not captured the local clustering in the network. So, let's go ahead and add a term to the model that will capture the tendency for nodes who are tied to have the same friends. For these models we will use our simple specification on grade homophily. There are a number of ways of specifying triadic effects like clustering and dominance. For example, one could include terms for different triad types (i.e., including different terms of the triad census). By including different combinations of triad counts, one could test different hypotheses about the formation of the network, comparing the fit of a 'clustering' model compared to a 'ranked clustering model'. This would be akin to a traditional tau statistic (see [Chapter 9](#ch9-Network-Centrality-Hierarchy-R)).

Similarly, we could capture clustering using a triangle term. A triangle is defined as all cases where i-j and j-k exist and then either k-\>i or i-\>k exist. Based on past work this is unlikely to be a good choice. In fact, it is very difficult to get a model with a triangle term to converge in the case of our school network, and we will not run this here.

Instead, let's include a gwesp term in the model, or a geometrically weighted edge-wise shared partner term. GWESP is a weighted count of the distribution of shared partners discussed above. If there is local clustering in the network, we would expect nodes who are tied to have many friends in common (and more than what we would expect based on chance expectations). Unlike the triangle term, GWESP is not focused solely on what happens in a given triad, as it counts all shared partners for a given ij pair. Note that we must specify a decay parameter, dictating the weighting on how much the first shared partners counts (in terms of increasing the probability of a tie) compared to the second, third, fourth, etc. Lower values put more weight on the initial partners and less weight on adding additional partners. Here we will set decay to 1. And to save time let’s limit the algorithm run time to three iterations.

A complete or empty network is not what you want to see. In general, there are two reasons we could have a degenerate model. 1. A poorly specified model. In a poorly specified model, the terms in the model do not reflect the actual processes that generated the network. When this happens, the simulated networks do not have realistic features, making it difficult to estimate the parameters very well. 2. The inputs that control the algorithm (like burnin) are not sufficient for the model to converge.

Maybe we should try a different specification of the model. For example, we can change the input decay value. Here we will set the decay value lower. When decay is high, the probability of a tie is greatly increased by adding another shared partner, even if the pair already have many shared partners. When decay is low, adding more shared partners does not greatly increase the probability of a tie if the pair already have a few shared partners. For this next model we will decrease the decay parameter to .5. We will also change some of the control parameters (like burnin, number of iterations and sample size) to let the algorithm run longer. This can take a bit to run, but we can try and speed that up using parallel processing (here utilizing 2 cores):

```{r message=F, warning=F}
# Increased burn-in and sample size for better convergence
mod_homoph_mutual_gwesp2 <- ergm(school_net ~ edges + 
                                   nodematch("gender_recode") +
                                   nodematch("grade") + 
                                   nodeifactor("gender_recode") + 
                                   nodeofactor("gender_recode") + 
                                   nodeicov("grade") + nodeocov("grade") +
                                   mutual + gwesp(decay = .5, fixed = T),
                                  control = control.ergm(MCMC.burnin = 100000, 
                                                        MCMC.samplesize = 8000, 
                                                        parallel = 2, 
                                                        parallel.type = "PSOCK")
                                 )
```

Now let’s look at model fit.

```{r results='hide'}
gof_mod_homoph_mutual_gwesp2 <- gof(mod_homoph_mutual_gwesp2, 
                                    GOF = ~ distance + espartners + model, 
                                    control = control.gof.ergm(seed = 108)) 
```

```{r}
par(mfrow = c(1, 3))
plot(gof_mod_homoph_mutual_gwesp2) 
```

As we can see, the fit is good (and improved) for the shared partner distribution. This suggests that our previous model (`mod_homoph_mutual2`) was, in fact, missing the local clustering in the network; without a gwesp term we overestimate the number of people who are tied together but have no common friends. This is now accurately captured in our new model (`mod_homoph_mutual_gwesp2`). We can also see that the fit, in terms of AIC and BIC, is better in the full model.

```{r}
summary(mod_homoph_mutual_gwesp2)
```

Substantively, we can see that after controlling for local clustering (gwesp) there is still a significant effect for grade homophily and reciprocity. The gwesp term is significant and clearly improved the fit of the model. A positive coefficient suggests that students tend to have more shared partners than we would expect based on density, homophily and reciprocity alone. More generally, the main micro drivers of tie formation in this network (sufficient to reproduce the macro features of the network) are grade homophily, reciprocity and local clustering. Moreover, net of other micro-processes, there would appear to be very little gender effects, as gender homophily is relatively weak and the nodefactor terms for gender are not significant (showing that boys don't have more ties net of other factors).

Let's simulate a network to see how it looks compared to the true network. This offers another useful check to see if the model is working as expected. The function is `simulate()`. The main arguments are:

-   object = the estimated ergm
-   nsim = number of simulated networks

Note that we can include constraints and control as with ergm, although the inputs must be set by `control.simulate.ergm()`:

```{r}
sim_schoolnet <- simulate(mod_homoph_mutual_gwesp2, nsim = 1, seed = 1012,
                          control = control.simulate.ergm(MCMC.burnin = 100000))
```

And now let's create a plot to compare the true network to the simulated one.

```{r fig.height=6.0, fig.width=9.0}
par(mfrow = c(1, 2))
plot(school_net, vertex.col = "grade", main = "True Network")
plot(sim_schoolnet, vertex.col = "grade", main = "Simulated Network") 
```

Looks reasonably close to the true network, but if we are unsatisfied, we may want to consider adding other terms to the model.

## Simulation

Simulation is often used, as above, as a means of checking model fit. Simulation can also be used as a useful tool in itself. In this case, we are not trying to estimate an ERGM. Instead, we will use the ERGM framework to simulate networks with desired properties. We can use the generated networks to ask theoretical questions. We can also use the generated network as inputs in other analyses (e.g., as inputs into a diffusion model-see [Chapter 14](#ch14-Network-Diffusion-R)).

Here, we ask an important theoretical question about homophily. What would happen to the network structure if we altered the strength of homophily? For example, if homophily on grade was weaker or stronger, how would transitivity in the network change? Our goal is to generate two networks, a 'strong' homophily network and a 'weak' homophily network, comparing the two cases to see how homophily affects network structure. We will continue to make use of the school network for this example. Both networks will be of the same size (n = 32) and density as the school network but they will differ on grade homophily. We could consider other features, like reciprocity, but we will keep this simple and just look at networks conditioned on density and grade homophily.

The simulation has three main steps: first, set the basic features of the network; second, estimate the ERGM coefficients; third, use the coefficients to generate networks. Note that the ergm package can handle quite complicated simulation inputs, allowing for realistic, nuanced networks to be generated with desired properties.

In the first step, we will create a network object, setting the size and type of network to be used in the simulation. Here, we initialize a network with the same size as our school network (32) and set it as directed.

```{r}
net <- network.initialize(n = 32, directed = T)
```

We will also seed the network with attributes, based on the distribution of grade from the observed data. This is necessary as we want to set the strength of grade homophily in the simulation (and thus nodes in the network must have a grade attribute).

```{r}
net %v% "grade" <- get.vertex.attribute(school_net, "grade")
```

In the second step, we will estimate an ERGM, which will serve as input into the simulation. The main inputs to the ERGM are the network of interest (constructed above), an ERGM formula and the target statistics, governing the features of the simulated networks. Here, the formula and target statistics are based on the number of edges and the number of edges that match on grade. We will need to set those target statistics for each case of interest, strong and weak homophily. Let's first take a look at the values in the observed network:

```{r}
summary(school_net ~ edges + nodematch("grade")) 
```

We can see that there are 124 edges in the original network and that 63 of the edges match on grade. For our analysis, we want to generate two networks, both with 32 nodes (the size of the original network) and 124 edges, but with different values for nodematch on grade. We will define strong homophily as a network with 75% of edges matching on grade, and weak homophily as a network with 25% of edges matching on grade. This means that nodematch on grade should be set to 93 edges for the strong homophily network (`.75 * 124`) and 31 for the weak homophily network (`.25 * 124`).

We will now estimate the ERGM for the strong homophily case. This will get us the coefficients to use in the simulation below. The call is similar to what we saw above but here we include a target.stats argument. target.stats corresponds to the desired counts for the terms included in the model (edges and nodematch on grade). Here we set edges to 124 and nodematch("grade") to 93. The target.stats input must be in the same order as the terms in the formula (so edges first and then nodematch on grade). Note that in this case one input is based on the empirical data (edges = 124) and one is based on a value set by the researcher (nodematch on grade = 93). The network of interest is net, constructed in step 1.

```{r message=F}
mod_stronghomophily <- ergm(net ~ edges + nodematch("grade"),
                            target.stats = c(124, 93)) 
```

```{r}
mod_stronghomophily
```

Now, let’s do the same thing for the weak homophily case. Here the number of edges is the same but only 31 edges go within grade.

```{r message=F}
mod_weakhomophily <- ergm(net ~ edges + nodematch("grade"), 
                          target.stats = c(124, 31))
```

As a third step, we will simulate networks based on the estimated models above. Let's start with the strong homophily model. We will generate a network of the right size (32), with approximately 124 edges and 93 of those going to people of the same grade. The main input is the estimated model. By default, the `simulate()` function will generate one network, but this can be altered using the nsim argument. We set the seed argument to make it easier to replicate.

```{r}
sim_strong_homophily <- simulate(mod_stronghomophily, seed = 1006) 
```

Let's check the network statistics of the simulated network:

```{r}
summary(sim_strong_homophily ~ edges + nodematch("grade")) 
```

It looks okay. The generated network has around 124 edges, with close to 75% matching on grade. The simulation is stochastic, so every network generated will have slightly different features. Note that we could also have constrained the simulation on the number of edges strictly, ensuring that the simulated networks have exactly 124 edges: `sim_strong_homophily <- simulate(mod_stronghomophily, constraints = ~ edges)`.

And now we simulate a network based on the weak homophily model.

```{r}
sim_weak_homophily <- simulate(mod_weakhomophily, seed = 1009) 
```

Let's check the network statistics of the weak homophily network:

```{r}
summary(sim_weak_homophily ~ edges + nodematch("grade")) 
```

Here we see that the generated network has around 25% edges matching on grade (again, this will vary from simulation to simulation). Let's plot the two simulated networks.

```{r fig.height=6.0, fig.width=9.0}
par(mfrow = c(1, 2))
plot(sim_weak_homophily, vertex.col = "grade", main = "Weak Homophily")
plot(sim_strong_homophily, vertex.col = "grade", main = "Strong Homophily")
```

We can see that the network with higher homophily is separated more clearly into social groups (based on grade). Let's also calculate transitivity in each of the generated networks.

```{r warning=F}
gtrans(sim_weak_homophily)
gtrans(sim_strong_homophily)
```

We can see that the network with stronger homophily has higher transitivity, so that a friend of a friend is more likely to be a friend. In this case, homophily on grade created groups in the network. Three people in the same grade are all likely to be friends, raising the potential for a friend of a friend to also be a friend. More generally, the results demonstrate how shifting patterns of homophily can affect network structure. We can imagine exploring a number of other questions using this kind of simulation platform; for example, we could ask how network cohesion changes as we alter the way actors form ties, based on mechanisms like homophily, hierarchy, or balance.

# 13, Part 2. Longitudinal Network Models: STERGM {#ch13-Longitudinal-Network-Models-STERGM-R .unnumbered}

This is the second tutorial for Chapter 13 on statistical network models. The first tutorial covered the case of [cross-sectional network data](#ch13-Cross-Sectional-Network-Models-ERGM-R). Here, we assume that a researcher has data on at least two time points and is interested in modeling change in the network over time. In this tutorial, we will walk through the estimation and interpretation of separable temporal exponential random graph models (STERGM). We can think of STERGM as a longitudinal extension to ERGM (see previous tutorial).

With STERG models, we are interested in predicting the formation and dissolution of edges from time T to T+1 (however defined). The model is appropriate for cases where the edges can be defined discretely, for the periods defined in the study. For example, we could predict the formation/dissolution of edges from data collected on relationships amongst students at two different time points. We will consider relational event models, appropriate for continuous-time, streaming data in the fourth tutorial ([Chapter 13, Part 4](#ch13-Relational-Event-Models-R)).

For this tutorial, we will use network data collected by Daniel McFarland on adolescents in a classroom. We have four discrete networks, capturing changes in network ties across different segments of a single class period. An edge exists in the first network if i and j talked in the first 10 minutes of the class (0-10 minutes); there is an edge in the second network if i and j talked in the second 10 minutes (10-20 minutes); there is an edge in the third network if i and j talked in the third 10 minutes (20-30 minutes); and there is an edge in the fourth network if i and j talked j in the last segment of the class (30-40 minutes). The networks are treated as undirected.

Substantively, our main question is how interaction partners in the classroom shift over a single class period. What mechanisms predict the adding of a tie that did not exist earlier in the class? What mechanisms predict the dropping of a tie? And are the mechanisms that predict the adding of a tie the same as dropping one? For example, students are likely to initiate conversations (i.e., add ties) with students who are sitting physically nearby. On the other hand, once two students are talking, sitting adjacent might not have a large effect on maintaining the tie over time; as they have already overcome the initial hurdle of being physically distant.

## Getting the Data Ready

We will make use of the **network** and **sna** packages in this tutorial (rather than **igraph**).

```{r message=F, warning=F}
library(sna)
library(network)
library(networkDynamic)
```

As a first step, we will load our four networks, already constructed as network objects.

```{r message=F, warning=F}
url1 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/talk_nets_undirected.RData"

load(url(description = url1))
```

Let's take a look at the four networks, `talk_time1`, `talk_time2`, `talk_time3` and `talk_time4`:

```{r}
talk_time1
talk_time2
talk_time3
talk_time4

# Visualize talk_time1-time4
plot(talk_time1,
     main = "Talk Network 1")

plot(talk_time2,
     main = "Talk Network 2")

plot(talk_time3,
     main = "Talk Network 3")

plot(talk_time4,
     main = "Talk Network 4")

```

`talk_time1` corresponds to interactions taking place in the first 'period' (0-10 minutes), `talk_time2` corresponds to interactions taking place in the second 'period' (10-20 minutes) and so on. Note that each of these networks already has attributes mapped onto them; specifically gender and race. In this case, all nodes are present in each period of observation. Each network (period 1 through 4) thus has the same set of nodes. In many cases, however, nodes may enter or exit through time (i.e., be present in period 1 but not period 2). We consider problems of entry/exit in the next lab, on two-mode networks.

Let's plot the networks for the first two periods. We will set the color of the nodes by gender (navy blue for male and light blue for female). We start by extracting gender from the first network.

```{r}
gender <- get.vertex.attribute(talk_time1, "gender")
```

```{r}
head(gender)
```

We now set color based on gender.

```{r}
cols <- ifelse(gender == "male", "navy blue", "light blue")
```

We will also set the layout for the plot to be the same across the two periods. This makes it easier to see how edges are dropped/added from period to period. We accomplish this by defining the locations (`locs`) to place the nodes and then using the coord argument in the plot statement. We define the locations of the nodes based on the period 1 network.

```{r fig.width=9.0, fig.height=6.0}
# Added a 2x2 layout to visualize all four talk networks in a single combined plot using a shared coordinate layout.

locs <- network.layout.fruchtermanreingold(talk_time1, layout.par = NULL)

# 2x2 plotting area
par(mfrow = c(2, 2))

# Plot 1
plot(talk_time1,
     main = "Talk Network 1 (0–10 min)",
     vertex.col = cols,
     coord = locs,
     vertex.cex = 2)

# Plot 2
plot(talk_time2,
     main = "Talk Network 2 (10–20 min)",
     vertex.col = cols,
     coord = locs,
     vertex.cex = 2)

# Plot 3
plot(talk_time3,
     main = "Talk Network 3 (20–30 min)",
     vertex.col = cols,
     coord = locs,
     vertex.cex = 2)

# Plot 4
plot(talk_time4,
     main = "Talk Network 4 (30–40 min)",
     vertex.col = cols,
     coord = locs,
     vertex.cex = 2)
```

We see that the basic structure of the network is pretty similar period to period, but specific edges do change. For example, in a number of instances students who were isolates (did not talk to anyone in the first 10 minutes) become active, socially, making connections to existing groups of talking students. We also see that one social group is almost entirely female, but otherwise boys and girls seem to mix pretty freely in this classroom.

STERG models require that our discrete, longitudinal networks be put together as a networkDynamic object. The function is `networkDynamic()`. In this case, the only input we need is a list of networks, where the networks are placed into a list in sequential order (see [Chapter 3](#ch3-Dynamic-Network-Data-R) for more complicated examples; e.g., where there are time changing vertex attributes or nodes becoming inactive/active over time). The resulting networkDynamic object will serve as input into our STERG model.

```{r message=F, results='hide', warning=F}
net_dynamic_4periods <- networkDynamic(network.list = list(talk_time1, 
                                                           talk_time2, 
                                                           talk_time3,
                                                           talk_time4))
```

```{r}
net_dynamic_4periods
```

Note that the default here is to set the start time (or onset) at 0 and the terminus at 4, defined as the period where no further change is recorded. We thus have a starting point of period 0 and changes occurring in period 1 , 2 and 3. Let's look at the object as a data frame.

```{r}
net_dynamic_4periods_dat <- as.data.frame(net_dynamic_4periods)
```

```{r}
head(net_dynamic_4periods_dat)
```

We can see that there is an edge between node 2 and node 11 in period 0 (onset) and this lasts to the end of the observation period. As another example, the edge between node 7 and 8 is present in period 0 (duration = 1), but is dropped during period 1.

We will also add an edge covariate to the networkDynamic object. Here we will include the seating arrangement in the classroom, as we might expect that students who sit close together are more likely to talk to one another. Let's read in the edgelist.

```{r}
url2 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/discrete_talk_nets_seating.txt"

seating_edgelist <- read.table(file = url2, header = T)
```

```{r}
head(seating_edgelist)
```

The data is stored as an edgelist, where i-\>j exists if node i is sitting near (i.e., adjacent) to node j. Let's turn the seating edgelist into a matrix and then add it to the networkDynamic object. We will first take the seating edgelist and turn it into a directed network object. We will then symmetrize it to make it undirected. We will use a 'weak' rule when symmetrizing the matrix, so if i is next to j or j is next to i, the matrix will have a 1 for both ij and ji (we do this to ensure that the matrix is logically consistent, so that if i is next to j, then j must be next to i). The `symmetrize()` function will output a matrix by default, which is what we want here. We will then attach the matrix to the networkDynamic object using `set.network.attribute()`.

```{r}
seating_network <- network(x = seating_edgelist, directed = T, 
                           vertices = data.frame(ids = 1:18)) 

seating_matrix <- symmetrize(seating_network, rule = "weak")

set.network.attribute(net_dynamic_4periods, "seating", seating_matrix)
```

## STERGMs

We are now ready to run an initial STERG model, where the goal is to predict the adding and dropping (or keeping) of edges across discretely defined time periods. This amounts to running separate ERGMs predicting the formation and persistence/dissolution of edges, given the network at time T. Note that the **tergm** package will allow us to choose if we want to model the keeping of an edge (persistence) or dropping of an edge (dissolution) for the persistence/dissolution portion of the model.

For the formation model, we run an ERGM predicting an edge between i-j in T+1, given that the i-j edge does not exist in the current network, time T. For the persistence model, we run an ERGM predicting an edge between i-j in time T+1, given that i-j does exist in time T. For the dissolution model, we run an ERGM predicting the absence of an edge between i-j in time T+1, given that i-j does exist in time T. In this way, STERGM is a direct extension of the exponential random graph models covered in the previous tutorial. The models are simply run with the addition of a time component, and corresponding conditioning to capture the formation and persistence/dissolution of edges. This means that the model specifications that work in the ERGM case will (mostly) be appropriate in the STERGM case. In this case, the network is undirected so we will only include terms appropriate for undirected networks.

Let's load the tergm package [@tergm]. Note this depends on **networkDynamic** and **ergm**. Let's also load **tsna**.

```{r message=F, warning=F}
library(tergm) 
library(tsna)
```

### Model 1: Just Edges

The main function is `tergm()`. The key arguments are:

-   formula = formula specifying the formation and persistence/dissolution equation
-   estimate = type of estimation to use in fitting model
-   constraints = formula specifying any constraints to put on model
-   control = list of inputs to control estimation algorithm; set using `control.tergm()`
-   times = periods to include in estimating model

Note that the formation and persistence/dissolution formulas are specified separately and do not need to be the same. The formation formula is set using `Form(~...)`. The persistence/dissolution formulas can be set using `Diss(~...)` or `Persist(~...)`. If we use Persist, then we will predict if the i-j edge remains from one period to the next. If we use Diss we predict the dissolution, or dropping, of edges from one period to next. For our example, we will focus on the persistence model. With the persistence model, positive effects make it more likely for an edge to last from one period to another. Note also that the constraints and control arguments work in very similar ways to what we saw in the ERGM case (see previous tutorial). For our first model, we will do something simple and only include a term for edges, capturing the base rate of tie formation/persistence. We set estimate to CMLE.

```{r message=F, warning=F, results='hide'}
stergm_mod1 <- tergm(net_dynamic_4periods ~ Form(~ edges) + Persist(~ edges),
                     estimate = "CMLE", times = 0:3)
```

```{r}
summary(stergm_mod1)
```

We can see that we have two different equations, one for tie formation and one for tie persistence. The results suggest that ties form at a lower rate than if people were randomly forming ties (based on the negative coefficient on edges for the formation model). To get a sense of what the results mean, we can look at the coefficient on edges for the tie persistence model. If we take the .4249, we can calculate the probability of an edge persisting from one period to the next:

```{r}
exp(.4249) / (1 + exp(.4249))
```

Thus, an edge that existed in one period has a .605 probability of still existing in the next period.

Note that we set times to 0:3, as we want to model the formation and persistence of edges across our four time periods. If we tried to run the following bit of code, without times explicitly set, we would run into problems: `wrong_mod1 <- tergm(net_dynamic_4periods ~ Form(~ edges) + Persist(~ edges), estimate = "CMLE")`

If we had run the dissolution model, we would get the same basic results but the .4249 would now be negative (as we are predicting the dropping of a tie):

```{r message=F, warning=F}
summary(tergm(net_dynamic_4periods ~ Form(~ edges) + Diss(~ edges), 
              estimate = "CMLE", times = 0:3))
```

### Model 2: Edges, Homophily and Nodefactor

We will now try a little more interesting model, including nodematch and nodefactor terms for gender and race. The nodefactor terms capture basic differences in degree by the nodal attribute of interest (i.e., do girls talk more than boys in class?), while the nodematch terms capture if there is homophily on the attribute (do girls tend to talk to other girls in class?). Here we will keep the formulas for formation and persistence the same.

```{r message=F, warning=F, results='hide'}
stergm_mod2 <- tergm(net_dynamic_4periods ~
                       Form(~ edges + nodematch("gender") + 
                              nodefactor("gender") +  
                              nodematch("race") + 
                              nodefactor("race")) + 
                       Persist(~ edges + nodematch("gender") + 
                                 nodefactor("gender") + 
                                 nodematch("race") + 
                                 nodefactor("race")), 
                     estimate = "CMLE", times = 0:3)
```

```{r}
summary(stergm_mod2)
```

Race and gender would not appear to play a large role in the formation or persistence of edges in this classroom network, although there is some evidence that students who identify as white initiate interactions at lower rates. The fit (based on BIC) is actually worse than in model 1, just including edges.

### Model 3: Edges, Nodefactor, and Seating

As a third model, we will incorporate seating arrangements into the formation and persistence models. The question is whether students who sit close to one another are more likely to initiate, and then maintain, interaction ties. Here, we will include an edgecov term with the seating matrix as the input. Note that we have already included the seating matrix on the networkDynamic object above (so no further manipulation is necessary here). Given our results above, we will drop most of the gender and race terms from the model, just keeping the nodefactor terms for race in the formation model and gender in the persistence model.

```{r message=F, warning=F, results='hide'}
stergm_mod3 <- tergm(net_dynamic_4periods ~ 
                        Form(~ edges +
                               nodefactor("race") +
                               edgecov("seating")) + 
                        Persist(~ edges + 
                                  nodefactor("gender") +
                                  edgecov("seating")),  
                      estimate = "CMLE", times = 0:3)
```

```{r}
summary(stergm_mod3)
```

### Model 4: Adding GWESP to the Model

For our last model, we will include a gwesp term, capturing if students form (and keep) edges when they share many common interaction partners. Here we will only include gwesp in the formation model, as including it in the persistence model leads to estimation problems.

```{r message=F, warning=F, results='hide'}
set.seed(107)

stergm_mod4 <- tergm(net_dynamic_4periods ~ 
                        Form(~ edges + 
                               nodefactor("race") +  
                               edgecov("seating") + 
                               gwesp(decay = .5, fixed = T)) + 
                        Persist(~ edges +
                                  nodefactor("gender") +
                                  edgecov("seating")),
                      estimate = "CMLE", times = 0:3, 
                      control = control.tergm(CMLE.ergm = 
                                           control.ergm(MCMC.burnin = 50000, 
                                                    MCMC.interval = 3000, 
                                                    MCMC.samplesize = 6500)))
```

The model with gwesp is estimated with MCMC estimation so we need to check the diagnostics, to make sure the model is converging. This is directly analogous to the kind of diagnostics we saw in the ERGM case.

There are two sets of diagnostics; first for the formation model and then for the persistence model. Looks okay on the whole, although we might consider rerunning with different input parameters or a simpler model. Let's take a look at the model results.

```{r}
summary(stergm_mod4)
```

The results suggest that students tend to form talking relationships when they have common interaction partners (looking at the positive, significant gwesp coefficient). So, if i and j talk with the same other students in Time T, they are likely to start talking with each other in T + 1.

## Checking Model Fit

Let's go ahead and check the fit of the model. One way of seeing if the model is fitting well is to simulate networks based on the underlying model and then compare the statistics in the simulated networks to that observed in the true networks (analogous to the `gof()` function in the ERGM case). Note that the simulations combine the formation and persistence processes and output the generated networks (based on both processes) at a given time slice. We will use a `simulate()` function. The main arguments are:

-   object = model of interest
-   nsim = number of separate replications; set to 1 in case of networkDynamic object
-   time.slices = number of distinct periods to run dynamic simulation over
-   nw.start = indicator for what network to start simulation at; nw.start=1 to begin with first observed network
-   monitor = formula indicating statistics to calculate on the simulated networks.

For our simulation we will set time.slices to 1000, so we simulate change over 1000 different time periods. Of course our actual data only has 4 time periods, but since the model is about the general tendencies of tie formation and persistence, having a larger number of time periods simply adds more information about how the model is fitting (i.e., can it generate realistic networks over many periods that correspond to what we saw in the actual data?). We will start the simulations at the time 1 network. We set monitor to edges and triadcensus (there are four types as the network is undirected). Note that triadcensus was not a term in the model. We use `set.seed()` to ease reproducibility.

```{r}
set.seed(130)
sim_mod4 <- simulate(stergm_mod4, nsim = 1, time.slices = 1000,
                     nw.start = 1, monitor = ~ edges + triadcensus(c(0, 1, 2, 3)))
```

If we take the generated object, `sim_mod4`, and use the `attributes()` function, we can grab the statistics for each generated network (specified in the monitor formula). This is housed under the `stats` part of the object.

```{r}
sim_stats <- attributes(sim_mod4)$stats
```

```{r}
head(sim_stats)
```

There are 1000 rows, 1 for each generated network. We see that there is a column for edges and four columns for the triadcensus, showing the count for edges and the four triad types (null triads, triads with 1 tie, triads with 2 ties and triads with 3 ties) for each simulated network (or time period in the larger simulation). This can be compared to what we saw in the observed network. We will use a `tErgmStats()` function to calculate the statistics on the observed network. The main arguments are:

-   nd = networkDynamic object
-   formula = ergm formula describing terms to calculate statistics on
-   start = period where calculations should start
-   end = period where calculations should end

We will set formula to include edges and triadcensus, matching what was calculated on the simulated networks above. We set start to 0 and end to 3, as we have 4 time periods, starting from 0 (onset).

```{r}
true_values <- tErgmStats(nd = net_dynamic_4periods, 
                          formula = '~ edges + triadcensus(c(0, 1, 2, 3))', 
                          start = 0, end = 3)
```

```{r}
true_values
```

Here we summarize the statistics of the simulated networks using `apply()` (summarizing over the columns):

```{r}
sim_values <- apply(sim_stats, 2, summary)
```

And now let’s add some useful rownames to the values calculated above.

```{r}
rownames(sim_values) <- paste("sim", rownames(sim_values), sep = "_")
```

```{r}
sim_values
```

Here we calculate the true values over the 4 periods as a means of comparison.

```{r}
true_values_mean <- colMeans(true_values)
```

Putting together the true and simulated values:

```{r}
rbind(sim_values, true_mean = true_values_mean)
```

Looks like the model is fitting okay, although the simulated networks have too many intransitive triads (`triadcensus.2`). We could imagine adjusting the model to account for that. As another way of assessing fit, we can extract networks from particular time periods and plot them against the observed network (we could also run a movie over all of the simulated networks). Here we will take the network from time period 10. We will use the `network.extract()` function, setting at to 10.

```{r}
net10 <- network.extract(sim_mod4, at = 10)
```

And now let’s plot the simulated network against the observed network, here just from the first 10 minute period.

```{r fig.width=9.0, fig.height=6.0}
par(mfrow = c(1, 2))
plot(talk_time1, main = "Observed Network from 0-10 Minutes")
plot(net10, main = "Example Simulated Network")
```

Looks okay on the whole, although we might consider adding additional homophliy terms to try and deal with the over count of intransitive triads.

Overall, our results suggest that seating arrangements structure the formation of interaction ties. Individuals sitting close together are likely to initiate interactions. Gender and race have relatively weak effects on the formation of ties. For example, even though there is one group of isolated girls, the overall effect of gender on formation of ties is not particularly strong. We also see that students tend to form new ties to people in their social cluster, with students initiating talking relationships with other students who have the same (current) partners as themselves. The tie persistence results are less clear, with few good predictors in the persistence model. In sum, the results suggest that classroom interactions are largely based on opportunity structure and norms of interaction: one tends to talk to other students in close proximity (socially or physically); once an initial discussion relationship is prompted, however, the interaction between i-j has its own internal dynamics largely independent of other, external factors.

# 13, Part 3. Two-mode Network Models {#ch13-Two-mode-Network-Models-ERGM-STERGM-R .unnumbered}

This is the third tutorial for Chapter 13, covering statistical network models in R. The first two tutorials covered network models for one-mode networks. [Part 1](#ch13-Cross-Sectional-Network-Models-ERGM-R) covered cross-sectional network models (ERGM), while [Part 2](#ch13-Longitudinal-Network-Models-STERGM-R) covered longitudinal network models (STERGM). In this tutorial, we will apply ERGMs and STERGMs to two-mode network data. Two-mode, or bipartite, networks are based on two types of actors (e.g., students and clubs). Ties exist between actors of different modes, but there are no ties between actors of the same mode. See [Chapter 11](#ch11-Two-mode-Networks) for details. We will draw heavily on the ERGM and STERGM tutorials, demonstrating how the specification and interpretation of the models must be rethought for two-mode networks. For example, even simple terms, like those capturing homophily, are not so simple when there are two kinds of actors in the network. We will cover cross-sectional models (ERGM) first, to get a sense of the model mechanics for two-mode networks. We will then cover longitudinal models (STERGM), appropriate for over time two-mode data.

Our substantive case is based on students joining clubs in a high school. We introduced this example in [Chapter 11](#ch11-Two-mode-Networks). We will work with data from two years, 1996 and 1997. We have basic information about the students, the clubs, and which students are members of which clubs for both years. Our goal is to tease out how students join, drop and keep their club affiliations through time. For example, are certain kinds of clubs more effective at keeping their members than others? Do students join a mixture of clubs (sports and academic) or do they stick to one kind of club (just sports)? And how strongly do students segregate themselves along racial lines? More generally, are the processes of tie gain (students joining clubs) the same as tie loss (students leaving clubs), and what does that tell us about the social dynamics of the school? In answering these questions, we will build on our results from [Chapter 11](#ch11-Two-mode-Networks), where we found, for example, that club memberships were partly segregated along racial lines, while generalist, service clubs served to integrate the school, which was otherwise divided along tracks of sports, academic interest, etc.

## Affiliation Data

We will make use of the network package for this tutorial, as well as the **ergm** and **tergm** packages.

```{r message=F, warning=F}
library(network)
library(ergm)
library(tergm)
library(networkDynamic)
```

Let's begin by reading in our data. We will first read in the affiliation matrices for the 1996 and 1997 school years, showing the club memberships for each student in the school. We have separate files for each year.

```{r}
url1 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/affiliations_1996.txt"

affiliations96 <- read.delim(file = url1, check.names = F)

url2 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/affiliations_1997.txt"

affiliations97 <- read.delim(file = url2, check.names = F)
```

Let's look at the first six rows and columns for 1996.

```{r}
affiliations96[1:6, 1:6]
```

Students are on the rows and clubs are on the columns. A 1 means that student i is part of club j in 1996 (e.g., the second student, 104452, is part of the symphonic marching band). Let's take a look at the dimensions of the affiliation matrices:

```{r}
dim(affiliations96)
dim(affiliations97)
```

We can see that there are 1295 students (on the rows) and 91 clubs (on the columns) for both 1996 and 1997. Note that the 1295 students reflects all students present in at least one year. Thus, students who are in the school in 1996 but leave (i.e., graduate) are in the 1996 and 1997 matrices. Similarly, students who enter in 1997, but are not in the school in 1996, are in both matrices. We will deal with node entry and exit later on.

And now we will read in the attribute data. We will read in a combined data frame, containing the attributes of both students and clubs. This was constructed in [Chapter 11](#ch11-Two-mode-Networks).

```{r}
url3 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/attributes_students_clubs.txt"

attributes_students_clubs <- read.delim(file = url3, stringsAsFactors = F)
```

```{r}
nrow(attributes_students_clubs)
```

Note that the data frame has 1386 rows, 1295 students + 91 clubs. Note also that the students come first in the data frame (the first 1295 rows), followed by the clubs. Let's look at the first six rows:

```{r}
head(attributes_students_clubs)
```

And here we look at the last six rows (using a `tail()` function):

```{r}
tail(attributes_students_clubs)
```

The data frame includes student attributes, like `race` (Asian, black, Hispanic, Native American, white), as well as club attributes like `club_profile` (how much attention does the club get? low, moderate, high, very high) and `club_type_detailed` (Academic Interest, Academic Competition, Ethnic Interest, Individual Sports, Leadership, Media, Performance Art, Service, Team Sports). For all student attributes, like `race` and `gender`, the clubs get NA values, while for all club attributes, like `club_profile`, the students get NA values. See [Chapter 11](#ch11-Two-mode-Networks) for more details. The following variables have meaningful values for both students and clubs: `ids` (a unique identifier), `type` (student or club), `missing96` (is node missing in 1996?), and `missing97` (is node missing in 1997?).

Let's do a quick table on type:

```{r}
table(attributes_students_clubs$type)
```

Before we construct our bipartite network, we need to create two more variables, which will be essential when running the models below. We will first construct a variable called, `studentgender_clubgender`. This simply combines the values for gender, for students, with the values for `club_type_gender`, for clubs. `gender` shows the gender of the specific student (male or female), while `club_type_gender` shows if the club is just boys, mixed gender, or just girls. By forming a single variable, it will be easier to adjust the models for the fact that certain types of students (e.g., female) are unlikely to join certain types of clubs (just boy clubs). The values for `gender` (students) comes first, followed by `club_type_gender` (clubs).

```{r}
gender <- attributes_students_clubs$gender
students <- attributes_students_clubs$type == "student"

club_type_gender <- attributes_students_clubs$club_type_gender
clubs <- attributes_students_clubs$type == "club"
  
studentgender_clubgender <- c(gender[students], club_type_gender[clubs])

attributes_students_clubs$studentgender_clubgender <- studentgender_clubgender
```

And now we do the same thing, combining student grade and club grade. `grade96` shows the grade of the student in the 1996 school year while `club_type_grade` shows which grades (if any) the club is restricted to. Again, the goal is to make it easier to adjust for the fact that certain students (e.g., those in grade 12) are unlikely to join certain types of clubs (eighth grade football). We do this for grade in both 1996 and 1997.

```{r}
grade96 <- attributes_students_clubs$grade96
grade97 <- attributes_students_clubs$grade97
 
club_type_grade <- attributes_students_clubs$club_type_grade

studentgrade96_clubgrade <- c(grade96[students], club_type_grade[clubs])
studentgrade97_clubgrade <- c(grade97[students], club_type_grade[clubs])

attributes_students_clubs$studentgrade96_clubgrade <- studentgrade96_clubgrade
attributes_students_clubs$studentgrade97_clubgrade <- studentgrade97_clubgrade
```

We are now in a position to construct our bipartite (two-mode) networks. Here, we will just construct the network using the 1996 data, as we will use the 1996 network in the cross-sectional models. We will consider network change, between 1996 and 1997, in the STERGM section below. Note that in constructing our network, the attribute data frame must be sorted to be consistent with the affiliation matrix. This means that the students in the attribute data frame should be sorted in the same order as the rows in the affiliation matrix, while the clubs in the attribute data frame should be sorted in the same order as the columns of the affiliation matrix (with the students coming first in the attribute data frame followed by the clubs). This is already done in our case. And now we turn our attribute data frame into a list, to make it easier to construct the network (as the network input is a matrix, meaning we cannot use the vertices argument in the `network()` function).

```{r}
attribute_list <- do.call(list, attributes_students_clubs)
```

Let's also make sure our affiliation data is treated as a matrix in R:

```{r}
affiliations96 <- as.matrix(affiliations96)
```

And now we construct the network, setting bipartite to T and using the affiliation matrix and the attribute list as inputs.

```{r}
bnet96 <- network(affiliations96, bipartite = T, 
                  vertex.attr = attribute_list)
```

We now have a two-mode network based on the 1996 school year. We will, however, need to clean up the network a bit before we can run any models. We need to drop all students who are not in the high school in 1996. In particular, the school includes grades 8-12, so all students in grades 6 or 7 in 1996 need to be removed (as they joined the school in later years). All clubs are present in both years, so we only have to worry about entry/exit for the students. We will utilize the `missing96` column in our combined data frame to remove those students not in the network in 1996.

We begin by creating a copy of our network (as we do not want to remove nodes from the original network). We then use a `delete.vertices()` function to remove the nodes of interest, in this case all nodes where `missing96` is equal to 1 (note that the `delete.vertices()` function will directly modify the input network).

```{r}
bnet96_nomiss <- bnet96
missing96 <- which(attributes_students_clubs[, "missing96"] == 1)
delete.vertices(bnet96_nomiss, missing96)
```

```{r}
bnet96_nomiss
```

We now have a two-mode network with 938 students and 91 clubs (1029 total). The students constitute the first mode while the clubs constitute the second mode (more generally, the rows are set as the first mode and columns as the second mode). A number of vertex attributes have also been added to the network, which will be useful when running the models.

## ERGM on a Two-mode Network

Our goal is to run a series of ERGMs on our two-mode network. The simplest option is to project the two-mode network into two one-mode networks, and analyze in the normal fashion. See [Chapter 13, Part 1](#ch13-Cross-Sectional-Network-Models-ERGM-R) for ERGMs on one-mode networks. Projecting the network is not an ideal solution, however, as this throws away important information, and can lead to incorrect inference. We will specify our models while maintaining both modes in the network. The network is defined by the ties between students and clubs, where a tie exists if student i is a member of club j. We are thus trying to predict which students are likely to be members of which clubs. The process of specifying an ERGM is similar to what we have already seen (in [Part 1](#ch13-Cross-Sectional-Network-Models-ERGM-R)), but the model terms are often a little different in the case of two-mode data. We will walk through the process of specifying two-mode ERGMs step-by-step, building up the model as we go.

### Edges

We will start with a simple model and just include a term for edges. The edges term is directly analogous to the edges term seen in previous tutorials, except here we must remember that edges can only exist between actors of different modes. As we have specified our network as bipartite, the model is estimated assuming this structure, where edges are only possible between students and clubs.

```{r warning=F, message=F}
mod1 <- ergm(bnet96_nomiss ~ edges)
```

```{r}
summary(mod1)
```

We can interpret the coefficient in terms of the baseline probability of a tie existing. The probability is: `exp(-3.44427) / (1 + exp(-3.44427))` = .0309402. Thus, about 3 percent of all possible edges actually exist. If we want to see where this comes from, we can calculate the probability directly by taking the affiliation matrix, summing up over all cells (to get the number of edges) and dividing by the total number possible (the number of rows times the number of columns).

```{r}
affil_mat96 <- as.matrix(bnet96_nomiss)
sum(affil_mat96) / (nrow(affil_mat96) * ncol(affil_mat96))
```

### Adjusting for Structural Issues: Gender and Grade Specific Clubs

We now turn to some tricky structural issues in the data. Most pressing is the fact that some student-club combinations cannot occur, or occur at extremely low rates. We need to adjust our model for these cases, or run the risk of biasing the estimates. For example, girls are very unlikely to join boys sports teams (e.g., wrestling or football), while boys are unlikely to join girls sports teams (volleyball). There are similar structural issues with grade, as some clubs are restricted to certain grades. For example, students in grade 9, 10, 11 or 12 do not join eighth grade sports teams (e.g., 8th grade football).

To account for these structural conditions, we will include nodemix terms in the model for gender and grade, capturing the student-club combinations that are unlikely to exist. The two attributes of interest are `studentgender_clubgender` and `studentgrade96_clubgrade`. By default, nodemix will include terms for all student-club combinations. We, however, only want to include terms for the rare cases of interest (e.g., boys joining girls sports). So, let's create two vectors, defining which terms we want to include for the grade and gender variables. We will start with gender. Remember that student gender is measured as male or female, while club gender is measured as boys, boys_girls, or girls. We want to print out the combinations of student-club genders (e.g., "male.boys"). By default, nodemix will print the terms sorted alphabetically, first by the second level (clubs) and then by the first level (students). Let's create our vector of names (student-club combinations) with that ordering in mind.

```{r warning=F, message=F}
student_gender <- sort(as.character(c("female", "male")))
club_gender <- sort(c("boys", "boys_girls", "girls"))

gender_levels <- paste(rep(student_gender, times = length(club_gender)), 
                      rep(club_gender, each = length(student_gender)),
                      sep = "." )
```

```{r}
data.frame(gender_levels)
```

The terms of interest are female.boys and male.girls; as these are the rare events we want to adjust for, girls joining boys clubs and boys joining girls clubs. This corresponds to spots 1 and 6 from the summary statistics, so let's create a vector holding that information.

```{r}
gender_terms <- c(1, 6)
```

And now we look at the grade attribute, `studentgrade96_clubgrade`. Student grade is measured as 8, 9, 10, 11 or 12. Club grade is measured as: eighth (just eighth graders), ninth (just ninth graders), ninth_tenth_eleventh (just ninth, tenth or eleventh graders), and ninth+ (no eighth graders). The value is all_grades if there are no restrictions on membership, in terms of grade. Let's get all student-club combinations for grade and put them together to be consistent with the nodemix term (sorted by the second level and then by the first level):

```{r}
student_grades <- sort(as.character(c(8:12)))
club_grades <- sort(c("all_grades", "eight", "ninth", "ninth_tenth_eleventh", "ninth+"))

grade_levels <- paste(rep(student_grades, times = length(club_grades)), 
                      rep(club_grades, each = length(student_grades)),
                      sep = "." )
```

```{r}
data.frame(grade_levels)
```

The grade variable is more complicated, but we want to include terms for any student-club combination that should not exist; like 12th graders in a ninth grade club, 12.ninth. Based on the order from the summary statistics, this corresponds to: 6 (10.eighth), 7 (11.eighth), 8 (12.eighth), 10 (9.eighth), 11 (10.ninth), 12 (11.ninth), 13 (12.ninth), 14 (8.ninth), 18 (12.ninth_tenth_eleventh), 19 (8.ninth_tenth_eleventh) and 24 (8.ninth+).

```{r}
grade_terms <- c(6, 7, 8, 10, 11, 12, 13, 14, 18, 19, 24)
```

We can now estimate our model, including nodemix terms for studentgender_clubgender and studentgrade96_clubgrade. We will specify which terms to include using the levels2 argument, setting it to the vectors defined above. To simplify the estimation of the model, we will specify these terms using an `Offset()` function (although we could have estimated the coefficients within the model). When using `Offset()`, the coefficients are not estimated and are instead set using the values supplied in the coef argument. This is appropriate in our case as the coefficients are based on logical conditions (e.g., 12th graders do not join 9th grade clubs) and can be set a priori by the researcher. Here, we set the coefficients to -10 for every term. We set the coefficients to -10 as we want to estimate the models conditioned on the fact that these student-club combinations are very rare.

```{r warning=F, message=F}
offset_coefs_gender <- rep(-10, length(gender_terms))
offset_coefs_grade <- rep(-10, length(grade_terms))

mod2 <- ergm(bnet96_nomiss ~ edges + 
               Offset(~ nodemix("studentgender_clubgender", 
                              levels2 = gender_terms), 
                      coef = offset_coefs_gender) + 
               Offset(~ nodemix("studentgrade96_clubgrade", 
                              levels2 = grade_terms), 
                      coef = offset_coefs_grade))
```

```{r}
summary(mod2)
```

All of the offset terms are set to -10, although they are not printed out here. We see that the edge coefficient is different than with `mod1`, suggesting the importance of adjusting our model for structural/logical constraints. Note also that the model fit should only be compared to other models with the same set of offset terms.

In this case, we used nodemix terms to capture structural conditions in the data, but we could imagine using nodemix terms to answer more substantive questions. We could test if certain types of students (e.g., girls) are more likely to join certain types of clubs (academic, leadership, etc.), although we will not consider this here.

## Second Mode Terms

We will now do something a little more interesting, and add nodefactor and homophily terms to our model. With two-mode networks, nodefactor and homophily terms can take two forms, with different terms for the first and second mode. Here we focus on the second mode, the clubs. We consider the first mode, students, below.

### Nodefactor

Nodefactor terms capture differences in degree across nodes, here clubs, with different attributes. We are interested in the main effect of club type (sports, academic, etc.) and club profile (low, moderate, high, very high) on membership. For example, do high profile clubs have more members than low profile clubs?

The term of interest is b2factor (b2 indicating the second mode of a bipartite network). We will include b2factor terms for each club attribute of interest, `club_type_detailed` and `club_profile`. We include a levels argument for `club_profile` to set the order that the results are printed. By default, the results are printed in alphabetical order. In this case, that would correspond to high (1), low (2), moderate (3), very high (4), with high excluded as the reference. But we want the results to run from moderate (3) to high (1) to very high (4), with low (2) excluded (as this is easier to interpret). For `club_type_detailed`, we use the levels argument to set the second category, academic interest, as the reference. We set levels to -2 to exclude only the second category.

```{r warning=F, message=F}
mod3a <- ergm(bnet96_nomiss ~ edges + 
               Offset(~nodemix("studentgender_clubgender", 
                              levels2 = gender_terms), 
                      coef = offset_coefs_gender) + 
               Offset(~nodemix("studentgrade96_clubgrade", 
                              levels2 = grade_terms), 
                      coef = offset_coefs_grade) + 
                b2factor("club_type_detailed", levels = -2) + 
                b2factor("club_profile", levels = c(3, 1, 4)))
```

```{r}
summary(mod3a)
```

We see, for example, that competitive academic clubs and individual sports tend to have fewer members than academic interest clubs (the reference), while service clubs (like National Honors Society) and team sports tend to be large. We also see that high profile clubs have, if anything, fewer members than low profile clubs. This largely reinforces the picture from [Chapter 11](#ch11-Two-mode-Networks), where we found that generalist service clubs occupied more central, integrating positions in the network. More formally, we can interpret the coefficient on individual sports (for example) as follows: the odds of a student being part of an individual sports team is `exp(-0.41835)` times lower than the odds of being part of an academic interest club. It is worth emphasizing that b2factor is based only on club attributes, and is thus different from nodemix (see above), which incorporates attributes from both modes.

### Homophily

We now turn to adding homophily terms to the model, still focusing on the attributes of the clubs. Homophily terms capture if actors tend to form ties with similar other actors. Homophily is more complicated with two-mode networks than with one-mode networks. This is the case as there are no direct ties from nodes of the same type; in our case, there are no ties from students to students or from clubs to clubs. So, if we are interested in homophily on a club attribute, say club type, we cannot ask if team sports are tied to other team sports, as there are no ties between clubs. Instead, we must ask if similar clubs are linked together through students; e.g., do students in team sports tend to be members of other team sports?

More technically, we must shift to counts of two-paths, based on homophily on the attribute of interest. The basic idea is to sum up the number of times that we see A-i-B, where A and B are clubs with the same attribute (e.g., both team sports) and i is a student in both A and B. The count is technically over half the number of two-paths, to avoid double counting (as A-i-B is substantively the same as B-i-A). The term is b2nodematch. A positive coefficient on b2nodematch would indicate that students are members of similar kinds of clubs, above what can be explained by other terms in the model.

One complication is that we must decide on how to sum up the homophilous two-paths. In the simplest case, we can sum up all of the two-paths that match on the attribute of interest. This is the default specification. We may, however, have good reason to incorporate some discounting, so that adding one more two-path (for a given edge) only marginally increases the count, once we reach a certain threshold. For example, if student i is a member of the football team, (i-football edge), then if i is also a member of the wrestling team, that would be a strong signal of matching on club type (both team sports). But adding another team sport membership, say i is also a member of the baseball team, may not offer quite as much information; as we already know that i joins team sports. We may then want to count the second two-path less than the first.

We can control the discounting using a beta parameter, which raises the count of two-paths (for a given edge) to beta. Setting beta to 1 would yield the number of two-paths (for an edge) where there is a match on the club attribute of interest (so the i-football edge would contribute a count of 2). Setting beta to 0 gives us the other extreme: showing if the given edge is involved in at least one homophilous two-path (so the i-football edge would contribute a count of 1). The count of two-paths, raised to beta, is then summed over all edges and divided by 2. See @bomiriya2014 for details.

For our model, we will include nodematch terms for club type and club profile. We set beta to .25, but we could imagine using a range of values (estimating the model each time), using model fit statistics to evaluate the choice of beta.

```{r warning=F, message=F}
set.seed(1007)

mod3b <- ergm(bnet96_nomiss ~ edges + 
                Offset(~nodemix("studentgender_clubgender", 
                              levels2 = gender_terms), 
                      coef = offset_coefs_gender) + 
                Offset(~nodemix("studentgrade96_clubgrade", 
                              levels2 = grade_terms), 
                      coef = offset_coefs_grade) + 
                b2factor("club_type_detailed", levels = -2) + 
                b2nodematch("club_type_detailed", beta = .25) + 
                b2factor("club_profile", levels = c(3, 1, 4)) + 
                b2nodematch("club_profile", beta = .25), 
              control = control.ergm(MCMC.burnin = 20000,
                                     MCMC.samplesize = 3000))

```

```{r}
summary(mod3b)
```

We can see that students do, in fact, tend to join clubs of the same type (based on the positive, significant coefficient for b2nodematch.club_type_detailed). Students in one academic competition club (debate) tend to be in other academic competition clubs (chess club). We see similar results for nodematch on club profile, as students in very high profile clubs (cheerleading) tend to be in other clubs that are very high profile (student council). This suggests something about the status hierarchy operating in the school, and offers a more explicit test than seen in [Chapter 11](#ch11-Two-mode-Networks). [Chapter 11](#ch11-Two-mode-Networks) showed that clubs tended to be grouped together along broad divides of sports, academic interest, etc., but here we can see more clearly how clubs of the same specific type and profile are tied at high rates. Note that these homophily coefficients are estimated net of the nodefactor terms. More generally, we see that two-mode ERGMs can differentiate between main effects of attributes (certain types of clubs have more members) and homophily (students join clubs with similar attributes).

## First Mode Terms

We now turn to adding the analogous terms (nodefactor and nodematch) for the first mode, students. Here we focus on student attributes, specifically for race. We will add b1factor and b1nodematch terms to the model (b1 indicating the first mode of a bipartite network). For b1factor, we test if there are differences in degree by racial groups (do white students join clubs at lower rates than Asian students?). For b1nodematch, we test if clubs are segregated along racial lines. We will count the number of two paths, i-A-j, where i and j are students of the same race and A is a club that both i and j are members of. Again, we can use the beta argument to set the discounting when summing up the two-paths that match on the attribute of interest (e.g., if student i joins club A and that creates 10 student to student two-paths that match on race, how should that 10 be discounted?). We will set beta to .15. Note, however, that if we set beta to 0 or 1 we would get the same basic calculations seen in the tutorial for [Chapter 11](#ch11-Two-mode-Networks), where we went over, step-by-step, how to calculate the number of matching two-paths on race. Section 11.6.1 corresponds to the case where beta is set to 1 (the total number of two-paths where students match on race), while Section 11.6.2 corresponds to setting beta to 0 (the number of student-club edges where students match racially with at least one other student in the club).

To help with estimation convergence, we will also set the reference category for the b1factor term to include Asian (1), Hispanic (3) and Native American (4) (basically collapsing some of the smaller categories into a single 'other' category). Finally, we will tweak the control parameters, increasing the burnin and sample size. This can take a little while to run, so we might want to include parallel processing options to speed things up (here we set the number of processors to 4).

```{r warning=F, message=F}
mod4 <- ergm(bnet96_nomiss ~ edges + 
               Offset(~nodemix("studentgender_clubgender", 
                              levels2 = gender_terms), 
                      coef = offset_coefs_gender) + 
                Offset(~nodemix("studentgrade96_clubgrade", 
                              levels2 = grade_terms), 
                      coef = offset_coefs_grade) + 
               b2factor("club_type_detailed", levels = -2) + 
               b2nodematch("club_type_detailed", beta = .25) + 
               b2factor("club_profile", levels = c(3, 1, 4)) + 
               b2nodematch("club_profile", beta = .25) +
               b1factor("race", levels = -c(1, 3, 4)) + 
               b1nodematch("race", beta = .15), 
             control = control.ergm(MCMC.burnin = 30000, 
                                    MCMC.samplesize = 5000, 
                                    parallel = 4, 
                                    parallel.type = "PSOCK"))
```

Let's go ahead and look at the results.

```{r}
summary(mod4)
```

It looks like black and white students are members of fewer clubs than Asian, Native American or Hispanic students (the reference), while there is segregation along racial lines (looking at the b1nodematch term). Students are unlikely to find themselves in clubs where there are few (or even no) students of the same race. For example, by chance we might expect Asian students (a small racial group) to often be in clubs with few other Asian students, but empirically this rarely happens. This offers a more formal test, and confirmation, of the basic results found in [Chapter 11](#ch11-Two-mode-Networks); unlike with Chapter 11, here we explicitly control for other factors, like differences in club size (across types of clubs) and differences in degree (number of affiliations) across racial groups. Looking at AIC and BIC, we see that the model fit is improved quite a bit from the previous model.

## Clustering

As a final set of models, we consider adding a term to capture clustering in the network. Here, we want to know if there are sets of students who join the same clubs. For example, we might expect a set of students to be joint members of chess, forensics and debate; another set might be part of cheerleading, yearbook, and student council. This opens up important questions about the patterns, or profiles, of club affiliations that students take on (we could also think of this in terms of identities). In ERGM-terms, we are asking if certain pairs of clubs share members at high rates; or, conversely, if certain pairs of students share a high number of clubs.

In previous labs, we included a GWESP term to capture clustering. GWESP is based on the shared partner count around edges. With two-mode networks, we must shift to terms that capture the non-edgewise shared partner distribution. We want to count the number of students shared by clubs (or vice versa), but clubs are not tied together; we thus need to consider the shared partners around non-edges. We can specify the count of shared partners separately, based on the two modes of interest. The term is gwb2nsp (geometrically weighted non-edgewise shared partner distribution) when we focus on the second mode. In our case, gwb2nsp captures the number of shared students between pairs of clubs. The term is gwb1nsp for the first mode. In our case, this counts the number of shared clubs between pairs of students. As with GWESP, we can set a decay parameter to discount higher counts of shared partners (i.e., students or clubs).

We need to read in the **ergm.terms.contrib** package to use the gwb1nsp or gwb2nsp terms, as these terms are user contributed (and currently not part of the main ergm package). Unfortunately, to use contributed ergm terms, we must install the package from source. This means that a user must have the required software already installed to compile the package (see <https://mac.r-project.org/tools/> for the Mac operating system or <https://cran.r-project.org/bin/windows/Rtools/> for Windows). We will also need the **remotes** package to install from the github location.

```{r warning=F, message=F, results=F}
remotes::install_github("statnet/ergm.terms.contrib")
library(ergm.terms.contrib)
```

The **ergm.terms.contrib** package should now be installed and loaded. This means that we can go ahead and run our ERGM with gwb2nsp or gwb1nsp as terms in the model. We will focus on gwb2nsp, looking at the clustering of students between pairs of clubs. For our first model, let's include edges, the offset terms and the gwb2nsp term. We will set the decay parameter to 3 for gwb2nsp. We set the decay parameter fairly high in this case because sharing a small number of students (or even a single student) between clubs may not be a strong sign of having shared clusters of students. We would likely set this lower if we were focusing on the first mode (as sharing 2 or 3 clubs would be a strong signal that two students join similar clubs).

```{r warning=F, message=F}
mod5a <- ergm(bnet96_nomiss ~ edges + 
               Offset(~nodemix("studentgender_clubgender", 
                              levels2 = gender_terms), 
                      coef = offset_coefs_gender) + 
                Offset(~nodemix("studentgrade96_clubgrade", 
                              levels2 = grade_terms), 
                      coef = offset_coefs_grade) + 
                gwb2nsp(3, fixed = T), 
              control = control.ergm(MCMC.burnin = 30000, 
                                     MCMC.samplesize = 5000, 
                                     parallel = 4, 
                                     parallel.type = "PSOCK"))
```

```{r}
summary(mod5a)
```

It looks like students do tend to cluster in sets of clubs. The positive coefficient for gwb2nsp suggests that clubs share students (or have common members) at a higher rate than if students were randomly part of clubs, given the baseline rate of club membership. Let's add in the nodematch and nodefactor terms for race (for students).

```{r warning=F, message=F}
mod5b <- ergm(bnet96_nomiss ~ edges + 
               Offset(~nodemix("studentgender_clubgender", 
                              levels2 = gender_terms), 
                      coef = offset_coefs_gender) + 
                Offset(~nodemix("studentgrade96_clubgrade", 
                              levels2 = grade_terms), 
                      coef = offset_coefs_grade) + 
                b1factor("race", levels = -c(1, 3, 4)) + 
                b1nodematch("race", beta = .15) + 
                gwb2nsp(3, fixed = T),
              control = control.ergm(MCMC.burnin = 30000, 
                                     MCMC.samplesize = 5000, 
                                     parallel = 4, 
                                     parallel.type = "PSOCK"))
```

```{r}
summary(mod5b)
```

It looks like clubs still share members at higher rates than expected by chance, controlling for racial segregation in club membership. As a final model, let’s add back in the nodefactor and nodematch terms for club attributes (club type and club profile). Here, we will change a few of the arguments in control.ergm to try and speed up the run time (e.g., relaxing the convergence criterion a bit). This can still take quite a bit to run.

## Two-mode Longitudinal Networks

We have so far run an initial set of cross-sectional models. We now turn to network change, where we want to model how students change club memberships over time. We thus want to predict when students add an edge (join a club) or drop an edge (quit a club). Here we will employ STERGMs, which we explored previously in [Part 2](#ch13-Longitudinal-Network-Models-STERGM-R). The basic idea is to run two models, formation and persistence. For formation, we predict the adding of an edge, given that the edge did not exist in the previous period; for persistence, we predict the keeping of an edge, given that the edge did exist in the previous period.

### Entry and Exit of Nodes

We will examine changes in club membership between the 1996 and 1997 school years. In examining over time change, we must deal with the fact that the student population is not constant from one year to the next. Students exit the network through graduation, with most students in grade 12 in 1996 no longer in the network in 1997. Students enter the network as they move from middle school (grades 6 and 7) to high school (8-12). Thus, those in grade 7 in 1996 would not be in the 1996 network, but would be in the 1997 network. We must deal with the problem of node entry/exit prior to estimating the model.

The simplest option is to remove any node that is not present in both years. This reduces the case base to just those nodes who have the opportunity to join and quit clubs during the observation period. This strategy has the advantage of offering a clear interpretation and is simple to implement. The downside is that we needlessly remove nodes, and thus edges, from the time 1 network, which the STERGM is ultimately conditioned on. For example, what is actually a clustering effect (shared partners between clubs) may look like a nodal effect (some clubs attract more members), if the students connecting the two clubs are removed from the network.

Alternatively, we can estimate the model while trying to retain information about the exiting nodes. The basic idea is to include all nodes in the analysis who are present in the first time period, 1996, even if they exit in 1997. This keeps the 1996 network exactly as we ran it above, avoiding any distortion from removing the exiting nodes. It also means that students who exit the school (i.e., graduate) will be technically present when constructing the 1997 network. We can deal with this problem by adding structural zeros to the model, indicating that all students who are missing in 1997 should have no ties to clubs. In this way, the exiting nodes inform the model only in providing information about the baseline network (from which we model ties being added or dropped).

We also need to deal with nodes who come into the school in 1997 (i.e., were 7th graders in 1996 and 8th graders in 1997). Nodes entering in 1997 have no edge information for 1996, while the model rests on being able to condition on the 1996 network. We thus remove all nodes from the analysis who enter in 1997. They would, however, have been kept in any model that examined the transitions between 1997 and 1998. Thus, nodes are incorporated into the model from the point in which they fully enter the setting.

In sum, the set of nodes in our 1996 network will match the set of nodes in our 1997 network, both consisting of all nodes present in 1996. Let’s go ahead and get our networks ready to run the models. The 1996 network is the same as above, so we just need to deal with 1997. Let's first create the 1997 network with all nodes included. In this case, the attributes are the same as with 1996, so we can use `attribute_list` again as input.

```{r}
affiliations97 <- as.matrix(affiliations97)
bnet97 <- network(affiliations97, bipartite = T, 
                  vertex.attr = attribute_list)
```

And now we remove all nodes from the 1997 network that are not present in 1996 (defined in `missing96`).

```{r}
bnet97_present96 <- bnet97
delete.vertices(bnet97_present96, missing96)
```

Let’s check and make sure that worked as expected. We will do a quick table on student grade for the 1997 network (after extracting the attribute using `get.vertex.attribute()`).

```{r}
table(get.vertex.attribute(bnet97_present96, "grade97"))
```

This looks right. There are no 8th graders but there is a grade '13', indicating that they have already graduated and are no longer in the school.

And now we can go ahead and create a networkDynamic object from our two networks. The networkDynamic object will serve as one of the main inputs to the STERGMs run below. Here, we will create a very simple networkDynamic object, using a list of already created networks (in order) as input.

```{r}
net_list <- list(bnet96_nomiss, bnet97_present96)
net_dynamic_9697 <- networkDynamic(network.list = net_list)
```

It will be useful to have a bit more information on the missing nodes in 1997 (i.e., those who were in the 1996 network but then exited). Nodes who are not present in 1997 cannot have memberships in clubs, and we need to treat any dyads involving those missing nodes as structural zeros for that year. To accomplish this, we need to know which dyads should be treated as missing for 1997.

We will create a matrix of 0s and 1s, 0 if an edge could exist between student and club and 1 if not. We will use an `outer()` function to create this matrix. The basic idea is to first grab two vectors, the type of node (student or club) and missing status in 1997 (0 = not missing and 1 = missing). We then take the student values for missing and add it to the club values for missing, doing this over all pairs of student-clubs using an `outer()` function. A 0 means that student and club are both present; a 1 means that one node in the dyad (student or club) is missing; and 2 means that both student and club are missing (although we do not see this in our data). We then set all values greater than 0 to missing, indicating that at least one node in the dyad is not present in 1997.

```{r}
type <- get.vertex.attribute(net_dynamic_9697, "type")
missing97 <- get.vertex.attribute(net_dynamic_9697, "missing97")
missing_dyads97 <- outer(missing97[type == "student"], 
                         missing97[type=="club"], '+')
missing_dyads97[missing_dyads97 > 0] <-  1
```

And now we use a `set.network.attribute()` function to attach our matrix to the networkDynamic object.

```{r}
set.network.attribute(net_dynamic_9697, "missing_dyad", missing_dyads97)   
```

We are now in a position to run our longitudinal network models.

## STERGM on a Two-mode Network

We will run a series of models to capture change in our two-mode network. For the first model, we will keep things simple and just estimate a baseline model of tie change. The model will include edges, as well as an offset term for the missing dyads. We include the missing dyad matrix as an edge covariate, setting the coefficient to -1e100, as these edges cannot exist in 1997 (we could set the coefficient to -Inf, but this can sometimes lead to problems when calculating the fit statistics). The model is kept the same for both the formation and persistence models. We again set the offset coefficients using `Offset()`. We set times to 0:1 as there are two time periods.

```{r message=F, warning=F, results='hide'}
offset_coef_miss <- -1e100

stergm_mod1 <- tergm(net_dynamic_9697 ~ 
                       Form(~ edges + Offset(~ edgecov("missing_dyad"), 
                                             coef = offset_coef_miss)) + 
                       Persist(~ edges + Offset(~ edgecov("missing_dyad"),
                                                coef = offset_coef_miss)), 
                     estimate = "CMLE", times = 0:1)

```

```{r}
summary(stergm_mod1)
```

This tells us that there is a `exp(-3.874) / (1 + exp(-3.874))` = .02 probability of a student joining a new club between 1996 and 1997 (looking at the formation model). Similarly, there is a `exp(-0.4184) / (1 + exp(-0.4184))` = .397 probability of staying in a club that one is currently a member of. These probabilities only pertain to students who have the opportunity to join and quit clubs between 1996 and 1997 (i.e., only those students who have values of 0 in the missing dyad matrix). Note that the fixed coefficients (for the missing dyad edge covariates) are not printed here.

We will now start to build up our model, first including offset terms for gender and grade specific clubs. Controlling for gender specific clubs is the same as before, but we need to update our offset term for grade (as there are no eighth graders in the 1997 network). We perform the same basic procedure as in the cross-sectional case, but here student grade runs from 9 to 13.

```{r}
student_grades_9697 <- sort(as.character(c(9:13)))

grade_levels_9697 <- paste(rep(student_grades_9697, times = length(club_grades)), 
                      rep(club_grades, each = length(student_grades_9697)),
                      sep = "." )
```

```{r}
data.frame(grade_levels_9697)
```

Let's identify which of those terms should be included in the model. Again, we want to include all combinations that are not possible (or at least very unlikely), such as a 12th grader in an eighth grade club (12.eighth). We will, however, not include any terms involving grade 13, as this is handled separately in the offset term for the missing dyads (as everyone in grade 13 is missing in 1997). We include terms for: 6 (10.eighth), 7 (11.eighth), 8 (12.eighth), 10 (9.eighth), 11 (10.ninth), 12 (11.ninth), 13 (12.ninth) and 18 (12.ninth_tenth_eleventh).

```{r}
grade_terms_9697 <- c(6, 7, 8, 10, 11, 12, 13, 18)
```

As above, we will set the coefficients for these terms to be -10. Let's go ahead and create a vector of coefficients (of -10s) to use in the `tergm()` function.

```{r}
offset_coefs_grade_9697 <- rep(-10, length(grade_terms_9697))
```

We are now ready to run our model, including controls for gender and grade specific clubs (note that we add the offset coefficients for gender and grade twice, once for the formation and once for the persistence models).

```{r message=F, warning=F, results='hide'}
stergm_mod2 <- tergm(net_dynamic_9697 ~ 
                       Form(~ edges + Offset(~ edgecov("missing_dyad"), 
                                             coef = offset_coef_miss) + 
                              Offset(~ nodemix("studentgender_clubgender", 
                                               levels2 = gender_terms), 
                                               coef = offset_coefs_gender) + 
                              Offset(~ nodemix("studentgrade97_clubgrade", 
                                               levels2 = grade_terms_9697), 
                                               coef = offset_coefs_grade_9697)) + 
                       Persist(~ edges + Offset(~ edgecov("missing_dyad"), 
                                                coef = offset_coef_miss) +  
                                 Offset(~ nodemix("studentgender_clubgender", 
                                                  levels2 = gender_terms), 
                                                  coef = offset_coefs_gender) + 
                                 Offset(~ nodemix("studentgrade97_clubgrade", 
                                                  levels2 = grade_terms_9697),
                                                  coef = offset_coefs_grade_9697)),
                     estimate = "CMLE", times = 0:1)
```

```{r}
summary(stergm_mod2)
```

Overall, our longitudinal models offer a number of insights into the school. We find that club attributes operate fairly consistently across the formation and persistence models. Students join clubs that are of the same type and profile, and leave if they are part of clubs that are incongruent along these lines. More generally, we see how club features serve to constrain students. Students join certain types of clubs and leave when they 'mistakenly' join incongruent ones (joining a sports team and performance art). This suggests that there are clear patterns of club memberships that students are allowed to take on, with students joining and dropping clubs to be consistent with such rules.

Homophily looks very different in the case of student attributes, with students joining, but not leaving, clubs based on the racial makeup. Demographic sorting is thus an impediment to integration, but once students overcome this initial sorting, the organization can potentially act to bring people together from different backgrounds. This is important as we might think that segregation along racial lines could be weakened (i.e., if a school took specific steps to create integrated clubs), while other kinds of segregation, based on patterns of club membership, status, etc., may be harder to alleviate.

There are a number of other questions we could address with these kinds of data and models. For example, we could add information about friendships between students. We could then ask if students recruit their friends into clubs, and whether they are more likely to stay if they have friends in the club. This would allow us to test the hypothesis we posed about racial homophily and club membership (that homophily arises in clubs because there is homophily in friendship and students recruit their friends into clubs). We could also examine the clustering of clubs between students, asking if pairs of students tend to join the same clubs (gwb1nsp instead of gwb2nsp). And similarly, we could add terms to capture differential homophily. We could ask, for example, if students in team sports are especially likely to join other team sports; compared to say, academic interest, where the tendency to match (i.e., join other academic clubs) might be weaker.

# 13, Part 4. Relational Event Models {#ch13-Relational-Event-Models-R .unnumbered}

This is the fourth tutorial for Chapter 13 on statistical network models. The first tutorial covered the case of [cross-sectional network data](#ch13-Cross-Sectional-Network-Models-ERGM-R). The second tutorial covered statistical models for discrete, [longitudinal networks](#ch13-Longitudinal-Network-Models-STERGM-R). The third tutorial covered statistical models for [two-mode networks](#ch13-Two-mode-Network-Models-ERGM-STERGM-R). Here, we will walk through relational event models, appropriate for continuous-time network data. Relational event models are based on micro-interaction data. The model assumes that there is time-stamped (or at least ordered) information on the interactions between a set of actors. This shifts the focus from discrete relationships (friend, advice, etc.) to the specific interactions between actors in a setting. The goal of the model is to predict what the next event is likely to be, based on the interactional tendencies, or rules, of behavior in the setting. Compare this to STERGM, where the goal is to predict the adding/dropping of ties from one period to the next, based on discretely defined networks.

Our data for this tutorial are based on streaming interaction data collected by Daniel McFarland on students in classrooms. Time-stamped interactions in each classroom were recorded, with information on the 'sender' and 'receiver' of the interaction, as well as the nature of the interaction. Interactions could be social or task-based, for example, although we focus just on social interactions in this tutorial. Data were collected across a large number of classrooms and days. See also [Chapter 3, Part 2](#ch3-Dynamic-Network-Data-R) (data processing for dynamic network data) and [Chapter 5](#ch5-Network-Visualization-R) (on visualization). Here we consider one classroom on two different days; both days are in the second semester of the year. We pick two days as a means of comparison. The first day was relatively uneventful and class was orderly. The second day was different, as there was a much higher rate of sanctioning behavior (i.e., disagreements between students and teacher on what was going on in the classroom). By examining two days, we see if the interactional signatures of order and disorder are different.

## Getting the Data Ready

Let’s begin by loading the main packages and getting the data ready to run the models.

```{r message=F, warning=F}
library(relevent)
library(sna)
```

**relevent** [@relevent] contains the functions to run relational event models.

Now, let's read in the interactional data for the first date. This is a data set reporting on the social interactions, i.e. talking, between individuals in the classroom (specifically classroom 182). This will serve as the outcome of interest, as we will predict what interactional features make certain events more likely to occur than others.

```{r}
url1 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class_interactions_date1.txt"

interactions_date1 <- read.table(file = url1, header = T)
```

Here we take a look at the first six rows of the data, for the main variables of interest: `send_col` (id of sender), `receive_col` (id of receiver), and `time_estimate_col` (time that interaction occurred).

```{r}
head(interactions_date1[, c("send_col", "receive_col", "time_estimate_col")])
```

Each row corresponds to an interaction between sender and receiver. For example, we can see that the third social interaction in this class period involved node 2 talking to node 5.

We need to manipulate the data a bit to get it in a form that the relational event model can use. For example, the events must be sorted in sequential order before we run any models. In this case, this is already done, but if it were not we would have to sort our data appropriately. Additionally, each interaction must also happen at a unique time period. The models are based on sequences of interactions, so a distinct order of events must be possible to establish. This means that relational event models are not so appropriate in cases where there are a large number of simultaneous events. For the sake of simplicity, we will remove all interactions directed from the teacher to all students or from all students to the teacher.

We can use the `to_all_col` and `from_all_col` to exclude these cases. `to_all_col` is equal to 1 if node i broadcasts to all other nodes simultaneously, while `from_all_col` is equal to 1 if node j receives from all nodes simultaneously. We will only keep those interactions where those variables are equal to 0 (i.e., i is not broadcasting to everyone in the class).

```{r}
not_to_all <- interactions_date1$to_all_col == 0
not_from_all <- interactions_date1$from_all_col == 0
interactions_date1 <- interactions_date1[not_to_all & not_from_all, ]
```

Now, in order to run the model, we need to create an edgelist (as a matrix object), where the first column is the time of the event, the second column is the sender and the third column is the receiver. Again, the events must be ordered sequentially.

```{r}
edgelist_date1 <- as.matrix(interactions_date1[, c("time_estimate_col",
                                                   "send_col", 
                                                   "receive_col")])
```

As a final data manipulation, we need to add a row to the end of the edgelist, showing the stop time where no more interactions are possible. Let's look at the end of the data frame:

```{r}
tail(edgelist_date1)
```

We can see that the last social interaction occurred at minute 43 in the class period. We will set the end of the interactional period at 43.10 (i.e., 6 seconds after the final interaction). To do this we add a row to the end of the edgelist, with the end time and then two NA values (for sender and receiver).

```{r}
edgelist_date1 <- rbind(edgelist_date1, c(43.10, NA, NA))
```

Now, we will read in some attribute data, as we want to use information on gender, grade, etc. as predictors in the model.

```{r}
url2 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class_attributes.txt"

attributes <- read.table(file = url2, header = T)
```

```{r}
head(attributes)
```

There are four variables: `id`, `gnd` (gender: 1 = male; 2 = female); `grd` (grade: 10 = 10; 11 = 11; 16 = teacher); `rce` (race: 3 = Black; 4 = White).

The relevent package requires that a researcher construct the node-level predictors as distinct columns (as opposed to using a `factor()` function within the formula). So, we will recode our variables to create the desired dummy variables of interest. In this case, we will create a predictor for whether the node is a teacher or not and for gender. We also need to create a term for the intercept. We will utilize the `recode()` function in the **car** package.

```{r message=F, warning=F}
library(car)
```

We first will create a variable for the intercept, which is a simple 1 for all nodes in the class.

```{r}
attributes$intercept <- 1
```

Now, we create a variable called `male` that is a 0 if `gnd` is equal to 2 (female) and 1 if `gnd` is equal to 1 (male).

```{r}
attributes$male <- recode(attributes$gnd, as.factor = F, "1 = 1; 2 = 0")
```

And here we do the same thing for `grd`, creating a binary variable called `teacher`. `teacher` is equal to 1 if they are a teacher (`grd` = 16) and 0 otherwise.

```{r}
attributes$teacher <- recode(attributes$grd, as.factor = F, 
                             "16 = 1; NA = NA; else = 0")
```

Finally, it will also be useful to have the size of the class handy. We can calculate that as the number of rows in the attribute data frame.

```{r}
class_size <- nrow(attributes)
```

## Running Initial Models

Relational event modeling is based on the logic of hazard models (or event history models), where the model predicts the risk of an event occurring (i.e., the hazard) as a function of different kind of interactional terms. There are a number of terms that we can include, including terms for baseline node effects (e.g., girls interact more than boys). We can also include terms that capture more micro-dynamics. These are labeled p-shifts, or participation shifts, and are only based on the previous event in the sequence. For example, if A talks to B, then we might expect the very next event to be B talking to A. The model allows us to include these different kinds of terms as a means of seeing what rules govern the interactions in the case of interest.

We are now in a position to run an initial relational event model. The function is `rem.dyad()`. The main arguments are:

-   edgelist = input edgelist in the form of time of event, sender, receiver
-   n = size of network
-   effects = vector with names of effects to be included in the model
-   covar = list of covariates that must correspond to the terms specified in effects
-   ordinal = T/F; T if data are ordinal (ordered but without specific time stamps); F if data include time specific information for each event

### Intercept Only Model

Our first model will be very simple and just includes an intercept, capturing the baseline rate for events to occur. We will set the effects to CovSnd. CovSnd is a basic sender effect, in this case initiating social interactions with others. We use the covar option to include specific terms for CovSnd. In this case we include the intercept (so all nodes are assumed to initiate interactions at the same rate). We set ordinal to FALSE as the data has time stamped information. Let's also set a seed to make it easier to replicate.

```{r message=F, results='hide'}
set.seed(1000) 

mod1 <- rem.dyad(edgelist = edgelist_date1, n = class_size, 
                 effects = c("CovSnd"), 
                 covar = list(CovSnd = attributes$intercept), 
                 ordinal = FALSE, hessian = TRUE)
```

And now we look at the results:

```{r}
summary(mod1)
```

The coefficient for the intercept (CovSnd.1) isn't all that interesting in itself, but it is important to understand what the coefficients mean (and how we can manipulate them) before moving to more complicated models. The first thing to note is that if we exponentiate this coefficient, we get the hazard of any event (i talking to j) occurring. Higher hazards mean the risk for an event occurring is higher. Second, if we multiple the hazard rate by the number of possible node pairs who could interact in a given moment, n \* (n - 1), we should get the expected number of interactions occurring per minute in the classroom. Finally, if we take the inverse of that (1 / number of interactions per minute), we get the expected amount of time between events, or the wait time between events.

```{r}
1 / (18 * 17 * exp(-4.057353))
```

The expected time between any event occurring is .189 standardized minutes (or .189 \* 60 = 11.34 seconds). And let's check this against the real data. We will take the total number of minutes for that class and divide that by the total number of interactions that occurred. We will define the total number of minutes as 43.1, the end time set above.

```{r}
total_classtime <- 43.1
```

We now define the total number of interactions. We will take the number of rows in the edgelist and subtract 1, as the last row is the stop time (not an interaction).

```{r}
num_interations <- nrow(edgelist_date1) - 1 
time_between_events <- total_classtime / num_interations
```

```{r}
time_between_events
```

We can see the estimate from the model approximates the raw data quite well.

### Adding Sender and Receiver Effects

We can do more substantively interesting things by incorporating the attributes of the nodes into the model. Let's first add a term for gender (coded as male = 1 and female = 0). We will add a sender effect, capturing whether males initiate fewer or greater interactions than females, as well as a receiver effect, capturing whether males receive fewer/greater interactions than females.

Here we create two matrices, one for sending and one for receiving. Each matrix will include the covariates we want to include for the sending or receiving effects. We start with the sender covariate matrix, where we will include variables for the intercept (it still must be included) and male.

```{r}
CovSnd1 <- cbind(attributes[, c("intercept", "male")])
```

And now we do the same thing for the receiver covariate matrix, including a variable for male (note that no intercept term is included here). Note that even though we already had male as a sender effect we need to include it separately as part of the receiver covariate matrix if we want to specify it as a receiver effect.

```{r}
CovRec1 <- cbind(attributes[, c("male")])
```

And now we are ready to estimate the model. The only difference from before is that we include "CovRec" in the effects and `CovRec1` as part of the covariate (covar) list.

```{r results='hide'}
mod2a <- rem.dyad(edgelist_date1, n = class_size, 
                  effects = c("CovSnd", "CovRec"), 
                  covar = list(CovSnd = CovSnd1, CovRec = CovRec1), 
                  ordinal = FALSE, hessian = TRUE)
```

```{r}
summary(mod2a)
```

We can see that the names of the variables are difficult to interpret. So, let's create a vector of more useful names, and put that on the outputted object.

```{r}
coef_names2a <- c("Intercept", "Sender_male", "Receiver_male")
```

And now we put those names on the coef part of the rem object and redo the summary of the model.

```{r}
names(mod2a$coef) <- coef_names2a
```

```{r}
summary(mod2a)
```

The summarized output now has more easily interpretable names for the variables. Let's go ahead and interpret the coefficient on sender_male. We can start by interpreting the sender_male coefficient in terms of hazards. By exponentiating the coefficient, we get the relative hazard for males to initiate the next interaction compared to females. exp(`r round(mod2a$coef[2], 6)`) = `r round(exp(mod2a$coef[2]), 3)`. This means that an event with males initiating has a hazard that is `r round(exp(mod2a$coef[2]), 3)` times lower than an event with females initiating.

Hazards themselves are bit hard to interpret. As an alternative, we can calculate mean wait times, or the expected time between events. Let's first calculate the expected time between two male-male events (where there is a male sender and a male receiver). Note that this calculation must incorporate all of the coefficients (intercept, Sender_male and Receiver_male). The calculation is the same as we did above with the intercept only model, where the expected wait is equal to: 1 / (dyads_at_risk \* hazard), where dyads_at_risk is the number of different ways that the event could occur, in this case the number of possible events that could involve two males.

Let's do a quick table to see how many males are in the classroom.

```{r}
table(attributes$male)
```

We can see that there are 12 females and 6 males. This means that there are 6 \* 5 different ways that we could have a boy as a sender and a boy as a receiver. We will use that in the calculation below:

```{r}
dyads_at_risk <- 6 * 5
```

Now we calculate the hazard. We take the coefficients, multiply them by the vector of input values (here setting Sender_male and Receiver_male to 1), sum it up and then exponentiate it.

```{r}
inputs <- c(intercept = 1, Sender_male = 1, Receiver_male = 1)
hazard_male_male <- exp(sum(mod2a$coef * inputs))
```

```{r}
hazard_male_male
```

And now to calculate wait time:

```{r}
1 / (dyads_at_risk * hazard_male_male)
```

This means that we would expect to wait `r round(1 / (dyads_at_risk * hazard_male_male), 3)` minutes between events that involve two boys. Now, let's do the same thing for girl-girl interactions. Here we set Sender_male to 0 and Receiver_male to 0.

```{r}
inputs <- c(intercept = 1, Sender_male = 0, Receiver_male = 0)
```

We define the dyads at risk to be 12 \* 11 as there are 12 females in the class.

```{r}
dyads_at_risk <- 12 * 11
```

Here we calculate the hazard.

```{r}
hazard_female_female <- exp(sum(mod2a$coef * inputs))
```

```{r}
hazard_female_female
```

We can see that the hazard for female-female events is higher than male-male events. And now for the wait time.

```{r}
1 / (dyads_at_risk * hazard_female_female)
```

We can see also that the wait time between female-female events is much lower. This is the case both because there are more females in the class and because males have a lower hazard of taking part in social interactions.

Now, let's add our teacher variable to the model. This is accomplished by creating new CovSnd and CovRec matrices that include the teacher variable.

```{r}
CovSnd2 <- cbind(attributes[, c("intercept", "male", "teacher")])
CovRec2 <- cbind(attributes[, c("male", "teacher")])
```

And now we rerun our model with the updated CovSnd and CovRec matrices.

```{r results='hide'}
mod2b <- rem.dyad(edgelist_date1, n = class_size, 
                  effects = c("CovSnd", "CovRec"), 
                  covar = list(CovSnd = CovSnd2, CovRec = CovRec2),  
                  ordinal = FALSE, hessian = TRUE)
```

Again, we can add better labels to the variable names and summarize the results.

```{r}
coef_names2b <- c("Intercept", "Sender_male", "Sender_teacher",
                  "Receiver_male", "Receiver_teacher")
names(mod2b$coef) <- coef_names2b
```

```{r}
summary(mod2b)
```

It looks like the teacher variables do not add much to the model. Let's compare the fit using BIC across the two models.

```{r}
mod2a$BIC - mod2b$BIC
```

Given that we want lower values, the simple model (`mod2a`, which just includes gender) would appear to be the better option.

### Adding Covariate Event Terms

As a third kind of term, we will consider adding covariate event terms to the model. Covariate events are predictors that are based on attributes of a dyad. Here, we will add the seating structure of the class to the model. The basic idea is that nodes who are close in the classroom are more likely to talk to one another. Let's first read in the data:

```{r}
url3 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class_seating_date1.txt"

seating_date1 <- read.table(file = url3, header = T)
```

```{r}
head(seating_date1)
```

As in the previous tutorial, the data is stored as an edgelist, indicating if node i is sitting adjacent to node j. The `rem.dyad()` function requires that this information be transformed into a matrix. So, we will go ahead and create a matrix of seating, where there is a 1 if i and j are next to each other in the class and 0 otherwise. We will accomplish this by taking the seating edgelist, turning it into a directed network object, and then symmetrizing it to make it undirected (this simply fixes any mistakes in the data where i may be recorded as being next to j but j is not recorded as being next to i). We will use a 'weak' rule when symmetrizing the matrix, so if i is recorded as sitting next to j or j is recorded as sitting next to i, the matrix will have a 1 for both ij and ji. Note that the `symmetrize()` function will output a matrix by default.

```{r}
seating_network_date1 <- network(x = seating_date1, directed = T, 
                                 vertices = data.frame(ids = 1:class_size)) 

seating_matrix_date1 <- symmetrize(seating_network_date1, rule = "weak")
```

```{r}
seating_matrix_date1
```

Now, we can run our model. We must include "CovEvent" in the effects input. We must also add the seating matrix to the list of covariates (covar), set with CovEvent. We will use the CovSnd and CovRec matrices that only include gender (so no teacher variable).

```{r results='hide'}
mod3a <- rem.dyad(edgelist_date1, n = class_size, 
                  effects = c("CovSnd", "CovRec", "CovEvent"),  
                  covar = list(CovSnd = CovSnd1, CovRec = CovRec1, 
                               CovEvent = seating_matrix_date1), 
                  ordinal = FALSE, hessian = TRUE)
```

Let's compare the fit between our previously preferred model and our new model.

```{r}
mod2a$BIC - mod3a$BIC
```

It looks like the seating arrangement does strongly shape what events occur in the classroom, as the fit is dramatically improved. Now, let’s go ahead and add a second covariate event matrix to the model. Here, we add information about the friendships that exist in the classroom. Friendship information was collected for each semester. Students were asked who they hung around with in the class. We will treat this information like a covariate event, with the idea that interactions during the class period are more likely to involve friends than non-friends. We will first read in the data for friendship during the second semester (when the class of interest took place).

```{r}
url4 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class_edgelist_sem2.txt"

friends_sem2 <- read.table(file = url4 , header = T)
```

```{r}
head(friends_sem2)
```

The edgelist captures if student i nominated student j as a friend. Note that the ids must match that found on the other data (interaction data, attributes, etc.). As before, we need to need to turn our edgelist into a matrix of 0s and 1s.

```{r}
friends_sem2_network <- network(x = friends_sem2, directed = T, 
                                vertices = data.frame(ids = 1:class_size)) 

friends_matrix_sem2 <- as.matrix(friends_sem2_network)
```

```{r}
friends_matrix_sem2
```

While it is relatively simple to include a single covariate event matrix in the model (see seating example above), it is a bit tricky to include multiple event matrices. The `rem.dyad()` function requires that multiple matrices first be put together as an p X n X n array, where p is the number of matrices and n is the size of the network.

We will go ahead and create that array. We will first create an array of NAs with the right structure (2 X 18 X 18).

```{r}
CovEvent_date1 <- array(data = NA, dim = c(2, class_size, class_size))
```

We will now put the first matrix, the seating matrix, in the first slot.

```{r}
CovEvent_date1[1, , ] <- seating_matrix_date1
```

We will now put the second matrix, the friendship matrix, in the second slot.

```{r}
CovEvent_date1[2, , ] <- friends_matrix_sem2
```

Checking the dimensions:

```{r}
dim(CovEvent_date1)
```

Note, that we would get unexpected (i.e., wrong) results if we had created the array to be 18 X 18 X 2. We are now in a position to run the model, putting in the newly created CovEvent array as input into the covar list.

```{r results='hide'}
mod3b <- rem.dyad(edgelist_date1, n = class_size, 
                  effects = c("CovSnd", "CovRec", "CovEvent"),
                  covar = list(CovSnd = CovSnd1, CovRec = CovRec1, 
                               CovEvent = CovEvent_date1), 
                  ordinal = FALSE, hessian = TRUE)
```

And let's add some useful names to the output (adding seating and friendship to the original vector of names) and summarize the results.

```{r}
coef_names3b <- c(coef_names2a, "Seating", "Friendship")
names(mod3b$coef) <- coef_names3b
```

```{r}
summary(mod3b)
```

We can see that both friendship and adjacent seating predict the occurrence of a social interaction event between i and j. Thus, an interaction event (i talks to j) is much more likely to occur if i sits next to j and/or i is friends with j. The gender differences in sending and receiving still seem to be present.

## Micro Rules of Interaction

So far we have built up a simple but plausible model of interactions in this classroom. Girls tend to talk more than boys, while friends and those sitting close to each other also tend to interact during class. What we have yet to capture is something about the 'rules' of interaction. For example, we might expect turn taking (i talks to j and then j talks to i) above what we can capture from friendship and seating effects alone.

We will build up our model slowly, including more complicated rules as we go along. As a start, let's include terms that capture recency of events. The two terms of interest are "RRecSnd" and "RSndSnd". With RRecSnd, we test if i is more likely to talk to j if j recently talked to i. With RSndSnd, we test if i is more likely to talk to j if i recently talked to j. The effects capture the idea that if j recently talked to i (or i recently talked to j), then an i-\>j event is more likely to be the next event. We will specify this model by including the "RRecSnd" and "RSndSnd" in the vector of effects. Note that we do not need to add anything to the covar list. For this first model we will not control for friendship or seating.

```{r results='hide'}
mod4a <- rem.dyad(edgelist_date1, n = class_size, 
                  effects = c("CovSnd", "CovRec", "RRecSnd", "RSndSnd"),
                  covar = list(CovSnd = CovSnd1, CovRec = CovRec1), 
                  ordinal = FALSE, hessian = TRUE)
```

Let's check the order of the coefficients using:

```{r}
names(mod4a$coef)
```

In this case, the recency effects are the first variables in the summary output. Let's set the names with this order in mind (adding the recency effects to the original vector of names).

```{r}
coef_names4a <- c("Recency_ji", "Recency_ij", coef_names2a)
names(mod4a$coef) <- coef_names4a
```

```{r}
summary(mod4a)
```

We see a positive coefficient on the recency receiver effects, suggesting that if j recently talked to i then i is likely to talk to j. On the other hand, there is a negative coefficient for the recency sender effects. This means that if i recently talked to j the next event is actually less likely to be i to j again. Note that the gender effects remain. Now we run the full model with seating and friendship included.

```{r results='hide'}
mod4b <- rem.dyad(edgelist_date1, n = class_size, 
                  effects = c("CovSnd", "CovRec", 
                              "CovEvent", "RRecSnd", "RSndSnd"), 
                  covar = list(CovSnd = CovSnd1, CovRec = CovRec1, 
                               CovEvent = CovEvent_date1), 
                  ordinal = FALSE, hessian = TRUE)
```

```{r}
coef_names4b <- c("Recency_ji", "Recency_ij", coef_names3b)
names(mod4b$coef) <- coef_names4b
```

```{r}
summary(mod4b)
```

The first thing to note is that the model fit is much improved from our previously preferred model.

```{r}
mod3b$BIC - mod4b$BIC
```

The second thing to note is that the gender coefficients are no longer significant. This suggests that controlling for both recency of events and seating explains the gender differences in social interaction. Girls are more likely to sit next to each other; couple this with the interactional tendencies to respond to a recent interaction, and we see why girls are more likely to be involved in events.

Now, let's see if we can consider other rules that may be shaping how nodes in this classroom interact with each other. We will now consider terms that capture p-shifts, or participation shifts. p-shifts are based strictly on the most recent event that occurred, rather than recency effects which can go back further in time. The idea is to capture micro rules in how interactions play out, based on the last interaction that took place. For our first example, we will add a turn taking rule, where A talks to B and the very next event is B talking to A. This is specified as "PSAB-BA" (as part of the effects vector). The rest of the model is the same as the previous model.

```{r results='hide'}
mod4c <- rem.dyad(edgelist_date1, n = class_size, 
               effects = c("CovSnd", "CovRec", "CovEvent", 
                           "RRecSnd", "RSndSnd", "PSAB-BA"), 
               covar = list(CovSnd = CovSnd1, CovRec = CovRec1, 
                            CovEvent = CovEvent_date1), 
               ordinal = FALSE, hessian = TRUE)
```

```{r}
coef_names4c <- c(coef_names4b, "PSAB_BA")
names(mod4c$coef) <- coef_names4c
```

```{r}
summary(mod4c)
```

We can see that the fit is improved greatly, as there is a large effect of turn taking in social interactions. We can also see that the effects for recency are much reduced from the previous model. Now, let's add a somewhat more complicated interactional rule. Here we will add a term for 'turn continuing', "PSAB-AY". This means that A talks with B and the very next event is A talking to someone else (besides B). We will also add a term for 'turn receiving', "PSAB-BY". Here, A talks to B and the very next event is B talking to someone else (besides A).

```{r results='hide'}
mod4d <- rem.dyad(edgelist_date1, n = class_size, 
                  effects = c("CovSnd", "CovRec", "CovEvent", 
                              "RRecSnd", "RSndSnd", "PSAB-BA", 
                              "PSAB-BY", "PSAB-AY"), 
                  covar = list(CovSnd = CovSnd1, CovRec = CovRec1, 
                               CovEvent = CovEvent_date1), 
                  ordinal = FALSE, hessian = TRUE)
```

```{r}
coef_names4d <- c(coef_names4b, "PSAB_BA", "PSAB_BY", "PSAB_AY")
names(mod4d$coef) <- coef_names4d
```

```{r}
summary(mod4d)
```

Looking at our two added terms, we see that only the PSAB-BY coefficient is significant (at traditional levels). This suggest that there are norms about whose 'turn it is' to talk. Once A talks to B, it is now B's turn to talk. They are very likely to talk back to A (PSAB-BA) but may also talk to another node (PSAB-BY). There is little evidence for nodes talking twice in a row to different people. It looks like the model fit is improved very slightly from the previous model:

```{r}
mod4c$BIC - mod4d$BIC
```

Let's look at one more set of terms, here focusing on interactional tendencies related to usurping the conversation. We include p-shift terms for "PSAB-XA" and "PSAB-XB". With PSAB-XA, A talks to B and then another node (X) usurps the conversation and answers A. With PSAB-XB, A talks to B and then another node (X) usurps the conversation and talks to B. In both cases, a node talks 'out of turn' relative to the previous event. Let's go ahead and add these terms to the model.

```{r results='hide'}
mod4e <- rem.dyad(edgelist_date1, n = class_size, 
                  effects = c("CovSnd", "CovRec", "CovEvent", "RRecSnd",
                              "RSndSnd", "PSAB-BA", "PSAB-BY",
                              "PSAB-AY", "PSAB-XA", "PSAB-XB"), 
                  covar = list(CovSnd = CovSnd1, CovRec = CovRec1, 
                               CovEvent = CovEvent_date1), 
                  ordinal = FALSE, hessian = TRUE)
```

Let's check the order of the coefficients:

```{r}
names(mod4e$coef)
```

And now we set the names to be consistent with the output.

```{r}
coef_names4e <- c(coef_names4b, "PSAB_BA", "PSAB_BY", "PSAB_XA", 
                  "PSAB_XB", "PSAB_AY")
names(mod4e$coef) <- coef_names4e
```

```{r}
summary(mod4e)
```

We can see that the PSAB-XB effect is much stronger than the PSAB-XA effect. This suggests that if one usurps the conversation from A to B, then one must interact with B in the next interaction, in essence giving B their rightful turn coming up next. And again, it looks like the model fit is improved a bit from the previous model.

```{r}
mod4d$BIC - mod4e$BIC
```

So, the overall story is one where there are clear micro rules to interacting in a classroom (above the effects for friendship, seating and so on). The basic rules could be summarized as: if A talks to B then the next event should be B to A; B to someone else; or someone else to B. Thus, there are clear norms about turn taking. If A talks to B, B is very likely to be part of the next event (one way or another). Of course, we could imagine looking at other kinds of terms, but this is pretty good start to the model.

## Assessing Model Fit

In interpreting our results, it is useful to see if our models are fitting well. While BIC can offer some evidence if one model is preferred to another, we can look at the residuals and the predicted classification to see how well the model is actually predicting the data. Here we will work with mod4e, our preferred model from above. One useful part of the output is `predicted.match`.

```{r}
head(mod4e$predicted.match)
```

Each row corresponds to an observed event. The first column shows if the model predicted the sender of that event correctly and the second column shows if the model predicted the receiver of that event correctly. Note that the model is trying to predict the specific sequence of events (i.e., the exact order of sender-receiver events). Let's see how we did by doing a table of the send and receive columns.

```{r}
send_col <- mod4e$predicted.match[, "send_col"]
receive_col <- mod4e$predicted.match[, "receive_col"]

table(send_col, receive_col)
```

We can see that 119 times we predicted the exact sender and the exact receiver correct (in sequence), while 89 times we got neither the sender nor the receiver correct. And now let's transform the table into proportions, showing the proportion where we get the exact sequence right (wrong, etc.).

```{r}
prop.table(table(send_col, receive_col))
```

We can see that about 52% of the time we get the exact event correct, while 39% of the time we miss completely and do not get the sender or receiver. The model is thus doing an okay job of prediction but is clearly missing some element that is important for predicting interaction events. Let's see if we can identify the cases (i.e., events) where the model is not doing such a good job at prediction. We first summarize the residuals for the model.

```{r}
summary(mod4e$residuals)
```

Now, we will identify some of the outlying cases, those with high residuals. We will define that for convenience as cases with residuals greater than 10.

```{r}
high_resids <- which(mod4e$residuals > 10)
```

Here we take a look at the events with high residuals, reducing the edgelist to just those cases where the model is not fitting well.

```{r}
edgelist_date1[high_resids, ]
```

Node 12 seems to show up quite a bit in the receiver column of these events. Let's take a look at the attributes for node 12.

```{r}
attributes[12, ]
```

Node 12 is the teacher in the class. So, perhaps we were too hasty in removing the teacher variable, as it looks like we are missing the set of interactions where students talk socially to the teacher. Let’s rerun our model but use `CovSnd2` and `CovRec2`, which includes both the gender and teacher effects.

```{r results='hide'}
mod4f <- rem.dyad(edgelist_date1, n = class_size,
                  effects = c("CovSnd", "CovRec", "CovEvent", 
                              "RRecSnd", "RSndSnd", 
                              "PSAB-BA", "PSAB-BY","PSAB-AY", 
                              "PSAB-XA", "PSAB-XB"), 
                  covar = list(CovSnd = CovSnd2, CovRec = CovRec2,
                               CovEvent = CovEvent_date1), 
                  ordinal = FALSE, hessian = TRUE)
```

Let's check the fit compared to the previous model (with no teacher sender/receiver effects).

```{r}
mod4e$BIC - mod4f$BIC
```

It looks like adding the teacher terms did help the fit a bit. And now let's look at the results.

```{r }
coef_names4f <- c("Recency_ji", "Recency_ij", "Intercept", 
                  "Sender_male", "Sender_teacher", 
                  "Receiver_male", "Receiver_teacher", 
                  "Seating", "Friendship", 
                  "PSAB_BA", "PSAB_BY", 
                  "PSAB_XA", "PSAB_XB", "PSAB_AY")

names(mod4f$coef) <- coef_names4f
```

```{r}
summary(mod4f)
```

We can see that the teacher is part of more interactions as the receiver (being talked to) than we would expect based on other terms in the model. This is likely the case because the teacher can easily talk to anyone in the class (i.e., the teacher is not subject to only talking to those adjacent to them in the classroom), and so the term on the seating arrangement pushes the previous model to under predict interactions with the teacher. This is now rectified in the current model.

## Comparison to a Second Date

We have so far run a number of models, interpreted the results and learned a bit about the interactional dynamics in this classroom. Here, we run through the same exercise (an abbreviated version) using interactional data from a different date. The classroom is the same, so the actors are the same, but this class takes place later in the second semester. More importantly, this was a date where there was a great deal more misbehaving in the class and the teacher had to sanction students to a much larger extent. Our question is how (or if) the interactional tendencies are different in a day where the class is less orderly and controlled. We begin by reading in the interactional data for this second date.

```{r}
url5 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class_interactions_date2.txt"

interactions_date2 <- read.table(file = url5, header = T)
```

And again, we will take out those interactions where one node talks to the entire class simultaneously.

```{r}
not_to_all2 <- interactions_date2$to_all_col == 0
not_from_all2 <- interactions_date2$from_all_col == 0

interactions_date2 <- interactions_date2[not_to_all2 & not_from_all2, ]
```

And now we create the edgelist matrix, adding a row for the stop time for the interactions (again, .10 standardized minutes after the last recorded interaction).

```{r}
var_names <- c("time_estimate_col", "send_col", "receive_col")
edgelist_date2 <- as.matrix(interactions_date2[, var_names])
```

```{r}
tail(edgelist_date2)
```

```{r}
edgelist_date2 <- rbind(edgelist_date2, c(33.10, NA, NA))
```

The friendship data is the same as above (corresponding to the second semester) but we need to read in the seating data for this day.

```{r}
url6 <- "https://github.com/JeffreyAlanSmith/Integrated_Network_Science/raw/master/data/class_seating_date2.txt"

seating_date2 <- read.table(file = url6, header = T)
```

```{r}
head(seating_date2)
```

Here we turn the edgelist into a matrix, as we did before.

```{r}
seating_network_date2 <- network(x = seating_date2, directed = T, 
                                 vertices = data.frame(ids = 1:class_size)) 

seating_matrix_date2 <- symmetrize(seating_network_date2, 
                                   rule = "weak")
```

And once again, we need to create a covEvent array with the new seating matrix.

```{r}
CovEvent_date2 <- array(data = NA, dim = c(2, class_size, class_size))
```

We will now put the first matrix, the seating matrix, in the first slot.

```{r}
CovEvent_date2[1, , ] <- seating_matrix_date2
```

We will now put the second matrix, the friendship matrix, in the second slot.

```{r}
CovEvent_date2[2, , ] <- friends_matrix_sem2
```

We are now in a position to run the same model as we did above. We will just run the preferred model (mod4f), with all terms included. Note that the CovRec and CovSnd matrices are the same as above (as the attributes are the same across time in this case).

```{r results='hide'}
mod4f_date2 <- rem.dyad(edgelist_date2, n = class_size, 
                        effects = c("CovSnd", "CovRec", 
                                    "CovEvent", "RRecSnd", "RSndSnd", 
                                    "PSAB-BA", "PSAB-BY","PSAB-AY", 
                                    "PSAB-XA", "PSAB-XB"), 
                        covar = list(CovSnd = CovSnd2, 
                                     CovRec = CovRec2, 
                                     CovEvent = CovEvent_date2), 
                        ordinal = FALSE, hessian = TRUE)
```

Let's add some more meaningful variable names. In, this case the terms are the same as with `mod4f`, so we can use those names directly.

```{r}
names(mod4f_date2$coef) <- coef_names4f
```

```{r}
summary(mod4f_date2)
```

We will create a little data frame to compare the coefficients from our two days.

```{r}
compare_coefs <- data.frame(date1 = mod4f$coef, date2 = mod4f_date2$coef)
```

```{r}
compare_coefs
```

Overall, much of the same interactional rules we saw above (in the 'normal' day) hold when looking at this second date, where the class was more unruly. We still see turn taking in interactions, for example (AB and then BA). We still see rules around usurping the conversation, such that when A talks to B and then X jumps in, they are likely to talk to B. Still, there would appear to be some potentially important differences (we would want to explore this more formally). For example, the effects for friendship and seating are particularly important for the second day. Similarly, there is some evidence that the tendency for PSAB-BY p-shifts are relatively high here. A class that is more unruly tends to have interactions that are based more on friendship and adjacent seating (i.e. talking to neighbors rather than doing discussion). Additionally, there may be a higher (relative) tendency for nodes to form a kind of two-step interaction (A-B-Y) rather than just a simple return to the person addressing them (A-B-A). This would potentially create more disruption in the classroom, as a larger number of students are brought into the initial interaction event.

It is also useful to compare these results to the kind of models we saw in the previous tutorial on STERGM. In general, STERG models allow us to test hypotheses about formation and persistence of ties. This opens up questions about triadic effects and larger group structures. In the language of relational event models, A may talk to B, and then B may talk to Y; but when B talks to Y, Y is very likely to be someone that A generally talks with. Thus, little groups in the classroom emerge that are harder to see in the relational event model than with STERGM. On the other hand, STERGM completely obscures the actual dynamics of moment-to-moment interactions, missing some of the ‘rules’ of interaction that come out so clearly in the relational event results.

Chapter 13 has covered statistical network models, moving from the cross-sectional case all the way up to continuous-time network data. In [Chapter 14](#ch14-Network-Diffusion-R), we still utilize statistical network models, but we focus on problems related to diffusion.
